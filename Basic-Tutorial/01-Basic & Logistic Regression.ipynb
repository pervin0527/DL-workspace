{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-Basic & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.벡터화(Vectorization)를 사용해야하는 이유."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서 사용하는 데이터들은 스칼라(단일 상수)보다는 벡터나 행렬과 같이 다차원 값을 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터의 내적을 계산할 때, 벡터가 가진 원소별로 곱하고 그 값들을 모두 더해야하는데,  \n",
    "일반적인 반복문을 통해 내적 연산을 구현하면 너무 비효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278, Computation Time : 0.032601000000020974ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "start_time = time.process_time()\n",
    "result = 0\n",
    "for i in range(len(x1)): ## 벡터의 내적은 두 벡터의 차원이 같을 때 가능한 연산.\n",
    "    result += x1[i] * x2[i]\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(f\"{result}, Computation Time : {1000 * (end_time - start_time)}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서 다차원 값을 이용한 계산은 Numpy 라이브러리를 많이 이용한다.  \n",
    "계산과정을 위한 코드가 훨씬 간단해지고, 속도도 훨씬 더 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278, Computation Time : 0.028043000000033125ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "start_time = time.process_time()\n",
    "result = np.dot(x1, x2)\n",
    "end_time = time.process_time()\n",
    "print(f\"{result}, Computation Time : {1000 * (end_time - start_time)}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sigmoid(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 스칼라를 입력 받는 형태의 함수 정의는 타입에러로 인해 다차원 값을 계산할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 입력으로 상수값만 가능한 형태의 구현.\n",
    "## 또는 단일 독립변수를 이용하는 함수라고 봐도 무방하다.\n",
    "def basic_sigmoid(x):\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999546021312976\n"
     ]
    }
   ],
   "source": [
    "print(basic_sigmoid(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x=[1,2,3]과 같이 리스트 타입의 입력은 불가능하다. 즉, 벡터나 행렬과 같이 다차원의 입력은 계산이 불가능하다.\n",
    "## 다변수함수(vector function) 형태의 구현이 필요함.\n",
    "\n",
    "x = [1, 2, 3]\n",
    "# basic_sigmoid(x) ## -> TypeError: bad operand type for unary -: 'list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이럴 때도 Numpy를 이용하는 것이 훨씬 효과적이다.  \n",
    "벡터(또는 행렬)을 입력하고 `np.exp(-x)`와 같은 연산을 적용하면 모든 원소에 지수함수 $e^x$가 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 벡터나 행렬과 같은 다차원 데이터를 입력으로 계산을 하기 위해서는 numpy 라이브러리를 활용한다.\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[9, 2, 5, 0, 0],\n",
    "               [7, 5, 0, 0 ,0]])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서의 학습이란 prediction과 ground-truth간 오차, loss를 계산하고 이를 Backpropagation함으로써  \n",
    "NN이 가진 Trainable parameter를 업데이트하는 방식으로 진행된다.\n",
    "\n",
    "여기서 \"업데이트\"라는 것은 해당 변수에 대한 손실값의 미분을 계산해 \"어떤 방향으로 변수의 값을 조정해야 손실값이 감소할까?\"를 알아내고,  \n",
    "손실값이 감소하는 방향으로 변수의 값을 조정하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ds = s * (1 - s)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(sigmoid_derivative(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.Download & Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 터미널에서 실행해 데이터셋을 다운로드 받습니다.\n",
    "\n",
    "    wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    tar -xvzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(files):\n",
    "    dataset = {'data': [], 'labels': []}\n",
    "    for file in files:\n",
    "        with open(file, 'rb') as fo:\n",
    "            data_dict = pickle.load(fo, encoding='bytes')\n",
    "            images = data_dict[b'data']\n",
    "            labels = data_dict[b'labels']\n",
    "            dataset['data'].extend(images)\n",
    "            dataset['labels'].extend(labels)\n",
    "    dataset['data'] = np.array(dataset['data'])\n",
    "    dataset['labels'] = np.array(dataset['labels'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "        \n",
    "dataset_dir = \"/home/pervinco/Datasets/cifar-10-batches-py\" ## 본인의 경로에 맞게 수정할 것.\n",
    "train_data_files = [\"data_batch_1\",\n",
    "                    \"data_batch_2\",\n",
    "                    \"data_batch_3\",\n",
    "                    \"data_batch_4\",\n",
    "                    \"data_batch_5\"]\n",
    "\n",
    "train_data_files = [f\"{dataset_dir}/{file}\" for file in train_data_files]\n",
    "test_data_file = [f\"{dataset_dir}/test_batch\"]\n",
    "\n",
    "trainset = unpickle(train_data_files)\n",
    "testset = unpickle(test_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2img(images):\n",
    "    X = []\n",
    "    for image in images:\n",
    "        ## 처음 1024개의 요소가 빨간색 채널, 다음 1024개의 요소가 녹색 채널, 마지막 1024개의 요소가 파란색 채널.\n",
    "        image = np.reshape(image, (3, 32, 32)) # 이미지를 (3, 32, 32)로 재구성\n",
    "        image = np.transpose(image, (1, 2, 0)) # # 축을 재배열하여 (32, 32, 3)으로 변환\n",
    "        image = image.astype(np.uint8)\n",
    "        X.append(image)\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "def cat_filter(labels, target=3):\n",
    "    Y = []\n",
    "    for label in labels:\n",
    "        if label == target:\n",
    "            Y.append([1])\n",
    "        else:\n",
    "            Y.append([0])\n",
    "\n",
    "    return np.array(Y).transpose((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y = vec2img(trainset[\"data\"]), cat_filter(trainset[\"labels\"])\n",
    "test_set_x_orig, test_set_y = vec2img(testset[\"data\"]), cat_filter(testset[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 50000\n",
      "Number of testing examples: m_test = 10000\n",
      "Height/Width of each image: num_px = 32\n",
      "Each image is of size: (32, 32, 3)\n",
      "train_set_x shape: (50000, 32, 32, 3)\n",
      "train_set_y shape: (1, 50000)\n",
      "test_set_x shape: (10000, 32, 32, 3)\n",
      "test_set_y shape: (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (3072, 50000)\n",
      "train_set_y shape: (1, 50000)\n",
      "test_set_x_flatten shape: (3072, 10000)\n",
      "test_set_y shape: (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize\n",
    "\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Build Computations & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return s\n",
    "\n",
    "def initialize_with_zeros(input_dim):\n",
    "    w = np.zeros((input_dim, 1)) ## 입력 데이터 샘플의 feature 수에 맞춰지기 때문.\n",
    "    b = 0.0\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    A = 1 / (1 + np.exp(-np.dot(w.T, X) - b)) ## sigmoid(w^Tx +b)\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) ## BCE\n",
    "\n",
    "    dw = 1/m * np.dot(X, np.transpose((A-Y))) ## Gradient of \\partial J / \\partial w\n",
    "    db = 1/m * np.sum(A-Y) ## Gradient of \\partial J / \\partial b\n",
    "\n",
    "    # cost = np.squeeze(np.array(cost))\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.1250040450043965\n",
      "cost = 0.15900537707692405\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        ## forward propagte\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        ## gradients\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        ## update trainable params\n",
    "        w = w - (learning_rate * dw)\n",
    "        b = b - (learning_rate * db)\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.80956046]\n",
      " [2.0508202 ]]\n",
      "b = 1.5948713189708588\n",
      "dw = [[ 0.17860505]\n",
      " [-0.04840656]]\n",
      "db = -0.08888460336847771\n",
      "Costs = [0.15900537707692405]\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4.Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = 1 / (1 + np.exp(-(w.T @ X + b)))\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Merging to Black Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.322767\n",
      "Cost after iteration 200: 0.318619\n",
      "Cost after iteration 300: 0.315850\n",
      "Cost after iteration 400: 0.313723\n",
      "Cost after iteration 500: 0.311992\n",
      "Cost after iteration 600: 0.310541\n",
      "Cost after iteration 700: 0.309304\n",
      "Cost after iteration 800: 0.308237\n",
      "Cost after iteration 900: 0.307307\n",
      "Cost after iteration 1000: 0.306490\n",
      "Cost after iteration 1100: 0.305766\n",
      "Cost after iteration 1200: 0.305121\n",
      "Cost after iteration 1300: 0.304543\n",
      "Cost after iteration 1400: 0.304021\n",
      "Cost after iteration 1500: 0.303549\n",
      "Cost after iteration 1600: 0.303119\n",
      "Cost after iteration 1700: 0.302725\n",
      "Cost after iteration 1800: 0.302364\n",
      "Cost after iteration 1900: 0.302031\n",
      "train accuracy: 89.932 %\n",
      "test accuracy: 89.96 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
