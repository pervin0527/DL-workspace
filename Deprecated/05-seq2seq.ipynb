{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from konlpy.tag import Mecab, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 처리 과정 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Q                         A  label\n",
      "0                       12시 땡!                하루가 또 가네요.      0\n",
      "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
      "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
      "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
      "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
      "...                        ...                       ...    ...\n",
      "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
      "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
      "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
      "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
      "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
      "\n",
      "[11823 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/ChatbotData.csv'\n",
    "df = pd.read_csv(data_dir) ## Question, Answer, Label(emotion - 0 : normal, 1 : negative, 2 : positive)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823 11823\n"
     ]
    }
   ],
   "source": [
    "question = df['Q']\n",
    "answer = df['A']\n",
    "\n",
    "print(len(question), len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('[^ ?,.!A-Za-z0-9가-힣+]')\n"
     ]
    }
   ],
   "source": [
    "## 이 문자들을 제외한 나머지들은 모두 제거한다.\n",
    "korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "normalizer = re.compile(korean_pattern)\n",
    "print(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일이랑 공부 병행 가능? ---> ['일', '이랑', '공부', '병행', '가능', '?']\n"
     ]
    }
   ],
   "source": [
    "## 한글 형태소 분석\n",
    "mecab = Mecab()\n",
    "okt = Okt()\n",
    "\n",
    "rand_idx = random.randint(0, len(question) - 1)\n",
    "print(f\"{question[rand_idx]} ---> {mecab.morphs(normalize(question[rand_idx]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일이 랑 공부 병행 가능 ?\n",
      "시간 활용 에 따라 다르겠죠 .\n"
     ]
    }
   ],
   "source": [
    "def clean_text(sentence, tagger):\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = tagger.morphs(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "print(clean_text(question[rand_idx], okt))\n",
    "print(clean_text(answer[rand_idx], okt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', 'ppl 심하네']\n",
      "['하루 가 또 가네요 .', '위로 해 드립니다 .', '여행 은 언제나 좋죠 .', '여행 은 언제나 좋죠 .', '눈살 이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "questions = [clean_text(sent, okt) for sent in question[:1000]]\n",
    "answers = [clean_text(sent, okt) for sent in answer[:1000]]\n",
    "\n",
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 사전 생성\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2count = {}\n",
    "        \n",
    "        self.word2index = {'<PAD>' : PAD_TOKEN,\n",
    "                           '<SOS>' : SOS_TOKEN,\n",
    "                           '<EOS>' : EOS_TOKEN}\n",
    "        \n",
    "        self.index2word = {PAD_TOKEN : '<PAD>',\n",
    "                           SOS_TOKEN : '<SOS>',\n",
    "                           EOS_TOKEN : '<EOS'}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 내 문제 는 뭘 까\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '내': 3, '문제': 4, '는': 5, '뭘': 6, '까': 7}\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(questions) - 1)\n",
    "print(f\"Original : {questions[rand_idx]}\")\n",
    "\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(questions[rand_idx])\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 37, 64, 65, 57, 14]\n"
     ]
    }
   ],
   "source": [
    "## batch를 위해 문장의 길이를 맞춰준다. padding\n",
    "max_length = 10\n",
    "sentence_length = 6\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,))\n",
    "sentence_tokens = sentence_tokens.tolist()\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 37, 64, 65, 57, 14]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = sentence_tokens[ : (max_length - 1)]\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "print(sentence_tokens)\n",
    "print(token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : [27, 37, 64, 65, 57, 14, 2, 0, 0, 0]\n",
      "total length: 10\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens.append(2) ## <EOS> token 추가.\n",
    "for i in range(token_length, max_length - 1):\n",
    "    sentence_tokens.append(PAD_TOKEN)\n",
    "\n",
    "print(f\"output : {sentence_tokens}\")\n",
    "print(f\"total length: {len(sentence_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Train, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, min_length=3, max_length=32):\n",
    "        super(TextDataset, self).__init__()\n",
    "\n",
    "        ## Token 정의.\n",
    "        self.PAD_TOKEN = 0\n",
    "        self.SOS_TOKEN = 1\n",
    "        self.EOS_TOKEN = 2\n",
    "\n",
    "        self.tagger = Mecab() ## 형태소 분석기\n",
    "        self.max_length = max_length ## 한 문장의 최대 길이\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "\n",
    "        self.source_clean = []\n",
    "        self.target_clean = []\n",
    "        self.wordvocab = WordVocab()\n",
    "        for _, row in df.iterrows():\n",
    "            source = row['Q'] ## raw questions\n",
    "            target = row['A'] ## raw answers\n",
    "\n",
    "            source = self.clean_text(source)\n",
    "            target = self.clean_text(target)\n",
    "\n",
    "            if len(source.split()) > min_length and len(target.split()) > min_length:\n",
    "                self.wordvocab.add_sentence(source)\n",
    "                self.wordvocab.add_sentence(target)\n",
    "                self.source_clean.append(source)\n",
    "                self.target_clean.append(target)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.source_clean[idx] ## n번째 (형태소로 분리된) 문장 하나를 가져온다. ex) 내 의견 을 존중 해줬으면\n",
    "        inputs_sentences = self.texts_to_sequences(inputs) ## ex) {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '내': 3, '의견': 4, '을': 5, '존중': 6, '해줬으면': 7}\n",
    "        inputs_padded = self.pad_sequence(inputs_sentences)\n",
    "\n",
    "        outputs = self.target_clean[idx]\n",
    "        outputs_sequences = self.texts_to_sequences(outputs)\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences)\n",
    "\n",
    "        return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_clean)\n",
    "\n",
    "\n",
    "    def normalize(self, sentence):\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        sentence = self.normalize(sentence)\n",
    "        sentence = self.tagger.morphs(sentence) ## 문장을 단어 단위로 분리한다.(형태소 분석)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        return sentence\n",
    "\n",
    "\n",
    "    def texts_to_sequences(self, sentence):\n",
    "        return [self.wordvocab.word2index[w] for w in sentence.split()] ## 각 형태소를 key로, index를 부여.\n",
    "\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens):\n",
    "        sentence_tokens = sentence_tokens[ : (self.max_length - 1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "        \n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "        for i in range(token_length, (self.max_length - 1)):\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "\n",
    "        return sentence_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tensor : torch.Size([50]) \n",
      " tensor([83, 84, 51, 85, 86, 18,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "Answer Tensor : torch.Size([50]) \n",
      " tensor([87, 88, 58, 89, 63, 90, 11,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 50\n",
    "dataset = TextDataset(\"./data/ChatbotData.csv\", min_length=3, max_length=MAX_LENGTH)\n",
    "x, y = dataset[10]\n",
    "print(f\"Question Tensor : {x.shape} \\n {x}\")\n",
    "print(f\"Answer Tensor : {y.shape} \\n {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8168 2042\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50]) torch.Size([16, 50])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1.embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50]) --> torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "embedding = nn.Embedding(num_embeddings=dataset.wordvocab.n_words, embedding_dim=embedding_dim) ## embedding할 단어의 수, embedding 차원\n",
    "\n",
    "embedded = embedding(x)\n",
    "print(f\"{x.shape} --> {embedded.shape}\")\n",
    "\n",
    "# embedded = embedded.permute(1, 0, 2)\n",
    "# print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 단어 사전의 개수 지정\n",
    "        self.num_vocabs = num_vocabs\n",
    "        # 임베딩 레이어 정의 (number of vocabs, embedding dimension)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # GRU (embedding dimension)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 단어사전 개수\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # 최종 출력은 단어사전의 개수\n",
    "        self.fc = nn.Linear(hidden_size, num_vocabs)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0) # (1, batch_size) 로 변환\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        output = self.fc(output.squeeze(0)) # (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "        decoder_input = torch.full((batch_size, ), SOS_TOKEN, device=self.device)\n",
    "\n",
    "        for t in range(0, output_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            predicted_outputs[t] = decoder_output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.argmax(1)\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "\n",
    "        return predicted_outputs.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(6417, 64)\n",
      "    (gru): GRU(64, 32)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(6417, 64)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(64, 32)\n",
      "    (fc): Linear(in_features=32, out_features=6417, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_DIM = 512\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.Inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "es = EarlyStopping(patience=10, \n",
    "                   delta=0.001, \n",
    "                   mode='min', \n",
    "                   verbose=True)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=2,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_func, device):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return avg_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x, y)\n",
    "            output_dim = output.size(2)\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            eval_loss += loss.item() * x.size(0)\n",
    "            \n",
    "    return eval_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_sentence(sequences, index2word):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "\n",
    "        word = index2word[p]\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)\n",
    "\n",
    "def random_evaluation(model, dataset, index2word, device, n=10):\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)      # Shuffle\n",
    "    sampled_indices = indices[:n]   # Sampling N indices\n",
    "    \n",
    "    # 샘플링한 데이터를 기반으로 DataLoader 생성\n",
    "    sampler = SubsetRandomSampler(sampled_indices)\n",
    "    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in sampled_dataloader:\n",
    "            x, y = x.to(device), y.to(device)        \n",
    "            output = model(x, y, teacher_forcing_ratio=0)\n",
    "            # output: (number of samples, sequence_length, num_vocabs)\n",
    "            \n",
    "            preds = output.detach().cpu().numpy()\n",
    "            x = x.detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(n):\n",
    "                print(f'질문   : {sequence_to_sentence(x[i], index2word)}')\n",
    "                print(f'답변   : {sequence_to_sentence(y[i], index2word)}')\n",
    "                print(f'예측답변: {sequence_to_sentence(preds[i].argmax(1), index2word)}')\n",
    "                print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 30.6700, val_loss: 1.1377\n",
      "[EarlyStopping] (Update) Best Score: 30.66996\n",
      "[EarlyStopping] (Update) Best Score: 17.14138\n",
      "[EarlyStopping] (Update) Best Score: 15.91111\n",
      "[EarlyStopping] (Update) Best Score: 15.32331\n",
      "[EarlyStopping] (Update) Best Score: 15.01190\n",
      "epoch: 6, loss: 14.7307, val_loss: 0.9392\n",
      "[EarlyStopping] (Update) Best Score: 14.73070\n",
      "[EarlyStopping] (Update) Best Score: 14.57431\n",
      "[EarlyStopping] (Update) Best Score: 14.47149\n",
      "[EarlyStopping] (Update) Best Score: 14.34900\n",
      "[EarlyStopping] (Update) Best Score: 14.16047\n",
      "epoch: 11, loss: 14.0677, val_loss: 0.9068\n",
      "[EarlyStopping] (Update) Best Score: 14.06774\n",
      "[EarlyStopping] (Update) Best Score: 14.01093\n",
      "[EarlyStopping] (Update) Best Score: 13.80560\n",
      "[EarlyStopping] (Update) Best Score: 13.72863\n",
      "[EarlyStopping] (Update) Best Score: 13.54630\n",
      "epoch: 16, loss: 13.4767, val_loss: 0.8917\n",
      "[EarlyStopping] (Update) Best Score: 13.47668\n",
      "[EarlyStopping] (Update) Best Score: 13.38095\n",
      "[EarlyStopping] (Update) Best Score: 13.28874\n",
      "[EarlyStopping] (Patience) 1/10, Best: 13.28874, Current: 13.31449, Delta: 0.02575\n",
      "[EarlyStopping] (Update) Best Score: 13.17842\n",
      "epoch: 21, loss: 13.1551, val_loss: 0.8866\n",
      "[EarlyStopping] (Update) Best Score: 13.15514\n",
      "[EarlyStopping] (Update) Best Score: 13.04827\n",
      "[EarlyStopping] (Patience) 1/10, Best: 13.04827, Current: 13.09684, Delta: 0.04856\n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[EarlyStopping] (Update) Best Score: 12.84051\n",
      "[EarlyStopping] (Patience) 1/10, Best: 12.84051, Current: 12.89949, Delta: 0.05898\n",
      "epoch: 26, loss: 12.8293, val_loss: 0.8877\n",
      "[EarlyStopping] (Update) Best Score: 12.82931\n",
      "Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[EarlyStopping] (Update) Best Score: 12.71796\n",
      "[EarlyStopping] (Patience) 1/10, Best: 12.71796, Current: 12.73698, Delta: 0.01902\n",
      "[EarlyStopping] (Patience) 2/10, Best: 12.71796, Current: 12.74341, Delta: 0.02545\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[EarlyStopping] (Patience) 3/10, Best: 12.71796, Current: 12.74226, Delta: 0.02430\n",
      "epoch: 31, loss: 12.6298, val_loss: 0.8909\n",
      "[EarlyStopping] (Update) Best Score: 12.62976\n",
      "[EarlyStopping] (Patience) 1/10, Best: 12.62976, Current: 12.71631, Delta: 0.08655\n",
      "Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[EarlyStopping] (Patience) 2/10, Best: 12.62976, Current: 12.72295, Delta: 0.09319\n",
      "[EarlyStopping] (Update) Best Score: 12.58876\n",
      "[EarlyStopping] (Patience) 1/10, Best: 12.58876, Current: 12.65366, Delta: 0.06490\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n",
      "epoch: 36, loss: 12.6470, val_loss: 0.8928\n",
      "[EarlyStopping] (Patience) 2/10, Best: 12.58876, Current: 12.64697, Delta: 0.05821\n",
      "[EarlyStopping] (Patience) 3/10, Best: 12.58876, Current: 12.65483, Delta: 0.06607\n",
      "[EarlyStopping] (Patience) 4/10, Best: 12.58876, Current: 12.65112, Delta: 0.06236\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[EarlyStopping] (Patience) 5/10, Best: 12.58876, Current: 12.66410, Delta: 0.07534\n",
      "[EarlyStopping] (Patience) 6/10, Best: 12.58876, Current: 12.69890, Delta: 0.11014\n",
      "epoch: 41, loss: 12.7265, val_loss: 0.8914\n",
      "[EarlyStopping] (Patience) 7/10, Best: 12.58876, Current: 12.72650, Delta: 0.13773\n",
      "Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[EarlyStopping] (Patience) 8/10, Best: 12.58876, Current: 12.70278, Delta: 0.11401\n",
      "[EarlyStopping] (Patience) 9/10, Best: 12.58876, Current: 12.62961, Delta: 0.04084\n",
      "[EarlyStopping] (Patience) 10/10, Best: 12.58876, Current: 12.59674, Delta: 0.00797\n",
      "[EarlyStop Triggered] Best Score: 12.58876\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "STATEDICT_PATH = '/home/pervinco/Models/seq2seq-chatbot-kor.pt'\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_dataloader, optimizer, loss_func, device)\n",
    "    \n",
    "    val_loss = evaluate(model, test_dataloader, loss_func, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), STATEDICT_PATH)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early Stop\n",
    "    es(loss)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "                   \n",
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "torch.save(model.state_dict(), f'/home/pervinco/Models/seq2seq-chatbot-kor-{best_loss:.4f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문   : 썸 타 다가 이제 안 볼 거 면 걍 잠수 타 ? 만나 서 말 해 ?\n",
      "답변   : 애매 함 이 좋 은지 생각 해 보 세요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 전 여친 카톡 메인 프로필 .\n",
      "답변   : 카톡 보 지 마요 .\n",
      "예측답변: 저 이 이 네요 .\n",
      "==============================\n",
      "질문   : 가슴 이 허락 하 질 않 네 .\n",
      "답변   : 마음 이 하 라는 대로 해야죠 .\n",
      "예측답변: 사랑 은 하 는 게 .\n",
      "==============================\n",
      "질문   : 결혼 준비 하 면서 못 보 던 모습 을 보 게 돼\n",
      "답변   : 본 모습 일지 도 몰라요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 여행 왔 는데 좋 아 하 는 선물 로 뭐 가 괜찮 을까 ?\n",
      "답변   : 여행지 에서 만 살 수 있 는 걸로 골라 보 세요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 취미 좀 만들 어 볼까 ?\n",
      "답변   : 뭐 든 시작 해 보 면 좋 을 거 예요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 마음 맞 는 사람 이 있 을까 ?\n",
      "답변   : 서로 에게 맞춰 갈 수 있 을 거 예요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 썸 을 오래 탔 는데 사귀 어도 되 는 거 야 ?\n",
      "답변   : 최대한 빨리 사귀 는 게 좋 을 거 같 아요 .\n",
      "예측답변: 사랑 은 하 는 게 좋 겠 어요 .\n",
      "==============================\n",
      "질문   : 그 시절 엔 다 그랬 지\n",
      "답변   : 추억 에 잠길 때 도 필요 해요 .\n",
      "예측답변: 저 이 이 네요 .\n",
      "==============================\n",
      "질문   : 이제 헤어진지 1 주일 됐 어\n",
      "답변   : 긴 시간 이 었 을 거 라 생각 해요 .\n",
      "예측답변: 좋 은 하 는 게 .\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
