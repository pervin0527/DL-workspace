{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder = \"/home/pervinco/Datasets/wikitext\"\n",
    "save_folder = \"/home/pervinco/Models/Wikit-Text\"\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "saved = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_folder):\n",
    "    os.makedirs(save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download WikiText Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wikitext(folder_dir):\n",
    "    wikitext2_url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip?ref=blog.salesforceairesearch.com\"\n",
    "    wikitext103_url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip?ref=blog.salesforceairesearch.com\"\n",
    "\n",
    "    def download_file(url, filename):\n",
    "        response = requests.get(url)\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    def unzip_file(zip_filename, extract_to):\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "\n",
    "    if not os.path.isdir(folder_dir) and not os.path.isfile(f\"{folder_dir}/wikitext-2/wiki.train.tokens\"):\n",
    "        print(\"Download Dataset.\")\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "        wikitext2_zip = os.path.join(download_folder, \"wikitext-2-v1.zip\")\n",
    "        wikitext103_zip = os.path.join(download_folder, \"wikitext-103-v1.zip\")\n",
    "\n",
    "        download_file(wikitext2_url, wikitext2_zip)\n",
    "        unzip_file(wikitext2_zip, download_folder)\n",
    "\n",
    "        download_file(wikitext103_url, wikitext103_zip)\n",
    "        unzip_file(wikitext103_zip, download_folder)\n",
    "        print(\"Done\")\n",
    "\n",
    "    else:\n",
    "        print(\"Dataset already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exist.\n"
     ]
    }
   ],
   "source": [
    "download_wikitext(download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{download_folder}/wikitext-2\"\n",
    "\n",
    "train_file = f\"{data_dir}/wiki.train.tokens\"\n",
    "valid_file = f\"{data_dir}/wiki.valid.tokens\"\n",
    "test_file = f\"{data_dir}/wiki.test.tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = read_file(train_file)\n",
    "valid_text = read_file(valid_file)\n",
    "test_text = read_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the <unk> <unk> of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . <unk> were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to <unk> 'l <unk> as soon as shotguns and rifles could be obtained from Little Rock instead of <unk> and lances , with which most of them were armed . Two days <unk> before the change could be effected . \" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_text[88])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', '<unk>', '<unk>', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', '<unk>', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', '<unk>', \"'\", 'l', '<unk>', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', '<unk>', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', '<unk>', 'before', 'the', 'change', 'could', 'be', 'effected', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "## 데이터셋에 포함된 각각의 문장들을 토큰화(문장을 단어 단위로 분리)하며, 마지막에 <eos> 토큰을 추가한다.\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def tokenize_file(text):\n",
    "    tokenized_data = []\n",
    "    for line in text:\n",
    "        tokens = tokenizer(line.strip()) + ['<eos>']\n",
    "        tokenized_data.append(tokens)\n",
    "    return tokenized_data\n",
    "\n",
    "train_data_tokens = tokenize_file(train_text)\n",
    "valid_data_tokens = tokenize_file(valid_text)\n",
    "test_data_tokens = tokenize_file(test_text)\n",
    "\n",
    "print(train_data_tokens[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28783\n",
      "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "## 토큰화된 데이터셋을 이용해 단어 사전을 생성한다.\n",
    "def build_vocab(data_tokens):\n",
    "    return build_vocab_from_iterator(data_tokens, specials=['<unk>', '<eos>'], min_freq=3)\n",
    "\n",
    "vocab = build_vocab(train_data_tokens + valid_data_tokens + test_data_tokens)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tokenized_data, vocab, batch_size):\n",
    "    data = []\n",
    "    for tokens in tokenized_data:\n",
    "        token_indices = [vocab[token] for token in tokens]\n",
    "        data.extend(token_indices)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.numel() // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, -1)\n",
    "    return data\n",
    "\n",
    "batch_size = 128\n",
    "train_data = get_data(train_data_tokens, vocab, batch_size) ## data.numel() : 2086708, num_batches : 16302, data : 218177\n",
    "valid_data = get_data(valid_data_tokens, vocab, batch_size) \n",
    "test_data = get_data(test_data_tokens, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.rnn(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.rnn.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.rnn.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "\n",
    "    # We don't learn the hidden state so we can detach it from the computation graph\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden = hidden.detach()\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65              \n",
    "tie_weights = True                  \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 33,700,975 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]             # The target is the src shifted by one batch\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)                 # model output\n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1583.341\n",
      "\tValid Perplexity: 999.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1528.389\n",
      "\tValid Perplexity: 1038.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1444.870\n",
      "\tValid Perplexity: 969.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1416.791\n",
      "\tValid Perplexity: 967.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1400.655\n",
      "\tValid Perplexity: 961.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1385.607\n",
      "\tValid Perplexity: 955.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1371.803\n",
      "\tValid Perplexity: 955.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1319.617\n",
      "\tValid Perplexity: 897.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1300.886\n",
      "\tValid Perplexity: 894.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1288.621\n",
      "\tValid Perplexity: 891.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1280.950\n",
      "\tValid Perplexity: 892.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1256.983\n",
      "\tValid Perplexity: 861.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1242.104\n",
      "\tValid Perplexity: 859.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1233.416\n",
      "\tValid Perplexity: 856.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1223.515\n",
      "\tValid Perplexity: 853.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1164.255\n",
      "\tValid Perplexity: 748.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 892.080\n",
      "\tValid Perplexity: 545.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 685.837\n",
      "\tValid Perplexity: 450.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 578.492\n",
      "\tValid Perplexity: 395.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 505.119\n",
      "\tValid Perplexity: 353.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 451.620\n",
      "\tValid Perplexity: 324.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 410.847\n",
      "\tValid Perplexity: 303.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 377.319\n",
      "\tValid Perplexity: 283.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 348.825\n",
      "\tValid Perplexity: 268.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 325.091\n",
      "\tValid Perplexity: 252.083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 304.497\n",
      "\tValid Perplexity: 241.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 286.438\n",
      "\tValid Perplexity: 230.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 271.973\n",
      "\tValid Perplexity: 223.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 259.396\n",
      "\tValid Perplexity: 216.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 248.490\n",
      "\tValid Perplexity: 210.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 238.444\n",
      "\tValid Perplexity: 206.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 229.572\n",
      "\tValid Perplexity: 199.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 221.671\n",
      "\tValid Perplexity: 195.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 213.954\n",
      "\tValid Perplexity: 189.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 207.312\n",
      "\tValid Perplexity: 188.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 201.064\n",
      "\tValid Perplexity: 184.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 195.015\n",
      "\tValid Perplexity: 180.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 189.734\n",
      "\tValid Perplexity: 178.406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 184.917\n",
      "\tValid Perplexity: 175.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 180.373\n",
      "\tValid Perplexity: 172.380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 176.056\n",
      "\tValid Perplexity: 169.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 171.931\n",
      "\tValid Perplexity: 170.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 165.732\n",
      "\tValid Perplexity: 165.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 163.636\n",
      "\tValid Perplexity: 164.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 161.376\n",
      "\tValid Perplexity: 164.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 159.630\n",
      "\tValid Perplexity: 162.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 158.016\n",
      "\tValid Perplexity: 161.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 156.218\n",
      "\tValid Perplexity: 161.440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 154.624\n",
      "\tValid Perplexity: 160.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 153.231\n",
      "\tValid Perplexity: 160.164\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load(f'{save_folder}/best.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "        valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "        \n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{save_folder}/best.pt')\n",
    "\n",
    "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  # temperature is unfair\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()     # take one sample from the distribution\n",
    "            \n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "think about a few of each other . we is a very lot of the whole of the season , but it is not only . it is a good to the\n",
      "\n",
      "0.7\n",
      "think about his free , . the story is the series of a person , as the third technology of the lifestream , which is a only sign of the way ,\n",
      "\n",
      "0.75\n",
      "think about his free , . the story is the series of a person , as the third technology of the lifestream , which is a only sign that the bones are\n",
      "\n",
      "0.8\n",
      "think about his free , . the story is the series of a person , as the third technology of the lifestream , which is a only sign that the bones are\n",
      "\n",
      "1.0\n",
      "think about his free drivers .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Think about'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# convert the code above into a for loop\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
