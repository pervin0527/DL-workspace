{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = \"hihello\"\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'h', 'i', 'l', 'o']\n",
      "{'e': 0, 'h': 1, 'i': 2, 'l': 3, 'o': 4}\n",
      "[1, 2, 1, 0, 3, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "char_set = sorted(list(set(sample_data))) ## unique한 문자를 골라냅니다.\n",
    "char_dict = {c : i for i, c in enumerate(char_set)} ## 각 문자에 고유한 label index를 부여합니다. {char : int_label}\n",
    "sample_idx = [char_dict[c] for c in sample_data] ## hihello 문자열에 대해 label index를 적용한 리스트를 만들어냅니다.\n",
    "\n",
    "print(char_set)\n",
    "print(char_dict)\n",
    "print(sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(char_set)\n",
    "hidden_dim = len(char_set)\n",
    "\n",
    "print(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 0, 3, 3]]\n",
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.]])]\n",
      "[[2, 1, 0, 3, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [sample_idx[:-1]] ## 마지막 문자 'o'를 제외한 나머지를 input으로 사용\n",
    "x_one_hot = [np.eye(input_dim)[x] for x in x_data] ## one_hot encoded vectors(stacked)\n",
    "\n",
    "y_data = [sample_idx[1:]] ## label은 반대로 첫번째 문자 'h'를 제외한 나머지를 사용하여 제대로 예측하는지 확인.\n",
    "\n",
    "print(x_data)\n",
    "print(x_one_hot)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 5]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(np.array(x_one_hot), dtype=torch.float32)\n",
    "Y = torch.tensor(y_data, dtype=torch.int64)\n",
    "h0 = torch.zeros(1, 1, hidden_dim) ## 첫번째 단계에서는 이전 단계 hidden state가 없기 때문에 0으로 초기화한다.\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "criterion = nn.CrossEntropyLoss() ## Multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 5]) torch.Size([1, 1, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 | loss : 1.650267481803894, prediction : eeehee\n",
      "epoch1 | loss : 1.4092960357666016, prediction : llelll\n",
      "epoch2 | loss : 1.276911735534668, prediction : lllllo\n",
      "epoch3 | loss : 1.2005442380905151, prediction : illllo\n",
      "epoch4 | loss : 1.139829397201538, prediction : ilillo\n",
      "epoch5 | loss : 1.0740702152252197, prediction : ilillo\n",
      "epoch6 | loss : 1.0043365955352783, prediction : ilillo\n",
      "epoch7 | loss : 0.9455127716064453, prediction : ilello\n",
      "epoch8 | loss : 0.9044656753540039, prediction : ilello\n",
      "epoch9 | loss : 0.8667243123054504, prediction : ilello\n",
      "epoch10 | loss : 0.8247909545898438, prediction : ilello\n",
      "epoch11 | loss : 0.7881093621253967, prediction : ilello\n",
      "epoch12 | loss : 0.7645301818847656, prediction : ihello\n",
      "epoch13 | loss : 0.73813796043396, prediction : ihello\n",
      "epoch14 | loss : 0.7148441672325134, prediction : ihello\n",
      "epoch15 | loss : 0.6978746056556702, prediction : ihello\n",
      "epoch16 | loss : 0.6842898726463318, prediction : ihello\n",
      "epoch17 | loss : 0.6715064644813538, prediction : ihello\n",
      "epoch18 | loss : 0.6584693193435669, prediction : ihello\n",
      "epoch19 | loss : 0.6453713774681091, prediction : ihello\n",
      "epoch20 | loss : 0.6328590512275696, prediction : ilello\n",
      "epoch21 | loss : 0.6214161515235901, prediction : ilello\n",
      "epoch22 | loss : 0.6113564372062683, prediction : ilello\n",
      "epoch23 | loss : 0.602939784526825, prediction : ilello\n",
      "epoch24 | loss : 0.5961698889732361, prediction : ilello\n",
      "epoch25 | loss : 0.5907108783721924, prediction : ilello\n",
      "epoch26 | loss : 0.5861366987228394, prediction : ilello\n",
      "epoch27 | loss : 0.582147479057312, prediction : ilello\n",
      "epoch28 | loss : 0.5785298943519592, prediction : ilello\n",
      "epoch29 | loss : 0.5751335620880127, prediction : ilello\n",
      "epoch30 | loss : 0.571890652179718, prediction : ilello\n",
      "epoch31 | loss : 0.5687688589096069, prediction : ilello\n",
      "epoch32 | loss : 0.5657665133476257, prediction : ilello\n",
      "epoch33 | loss : 0.5629237294197083, prediction : ilello\n",
      "epoch34 | loss : 0.5602818727493286, prediction : ilello\n",
      "epoch35 | loss : 0.5578752160072327, prediction : ilello\n",
      "epoch36 | loss : 0.5557209253311157, prediction : ilello\n",
      "epoch37 | loss : 0.5538041591644287, prediction : ilello\n",
      "epoch38 | loss : 0.5521090030670166, prediction : ilello\n",
      "epoch39 | loss : 0.5506106019020081, prediction : ilello\n",
      "epoch40 | loss : 0.5492741465568542, prediction : ilello\n",
      "epoch41 | loss : 0.5480793118476868, prediction : ilello\n",
      "epoch42 | loss : 0.5470029711723328, prediction : ihello\n",
      "epoch43 | loss : 0.5460338592529297, prediction : ihello\n",
      "epoch44 | loss : 0.5451641082763672, prediction : ihello\n",
      "epoch45 | loss : 0.5443788170814514, prediction : ihello\n",
      "epoch46 | loss : 0.5436685681343079, prediction : ihello\n",
      "epoch47 | loss : 0.5430161356925964, prediction : ihello\n",
      "epoch48 | loss : 0.542412519454956, prediction : ihello\n",
      "epoch49 | loss : 0.5418470501899719, prediction : ihello\n",
      "epoch50 | loss : 0.5413145422935486, prediction : ihello\n",
      "epoch51 | loss : 0.5408121943473816, prediction : ihello\n",
      "epoch52 | loss : 0.540337085723877, prediction : ihello\n",
      "epoch53 | loss : 0.5398887395858765, prediction : ihello\n",
      "epoch54 | loss : 0.5394627451896667, prediction : ihello\n",
      "epoch55 | loss : 0.539057195186615, prediction : ihello\n",
      "epoch56 | loss : 0.5386660695075989, prediction : ihello\n",
      "epoch57 | loss : 0.538287341594696, prediction : ihello\n",
      "epoch58 | loss : 0.537916362285614, prediction : ihello\n",
      "epoch59 | loss : 0.5375515818595886, prediction : ihello\n",
      "epoch60 | loss : 0.5371889472007751, prediction : ihello\n",
      "epoch61 | loss : 0.5368247628211975, prediction : ihello\n",
      "epoch62 | loss : 0.5364516973495483, prediction : ihello\n",
      "epoch63 | loss : 0.5360601544380188, prediction : ihello\n",
      "epoch64 | loss : 0.5356347560882568, prediction : ihello\n",
      "epoch65 | loss : 0.5351527333259583, prediction : ihello\n",
      "epoch66 | loss : 0.5345767140388489, prediction : ihello\n",
      "epoch67 | loss : 0.5338425636291504, prediction : ihello\n",
      "epoch68 | loss : 0.5328284502029419, prediction : ihello\n",
      "epoch69 | loss : 0.531286895275116, prediction : ihello\n",
      "epoch70 | loss : 0.52866530418396, prediction : ihello\n",
      "epoch71 | loss : 0.5236398577690125, prediction : ihello\n",
      "epoch72 | loss : 0.5132597088813782, prediction : ihello\n",
      "epoch73 | loss : 0.4968625009059906, prediction : ihello\n",
      "epoch74 | loss : 0.5922738909721375, prediction : ihello\n",
      "epoch75 | loss : 0.5213331580162048, prediction : ihello\n",
      "epoch76 | loss : 0.5496389865875244, prediction : ihello\n",
      "epoch77 | loss : 0.5628877878189087, prediction : ihello\n",
      "epoch78 | loss : 0.5675554275512695, prediction : ihello\n",
      "epoch79 | loss : 0.5669728517532349, prediction : ihello\n",
      "epoch80 | loss : 0.5616148710250854, prediction : ihello\n",
      "epoch81 | loss : 0.5505841374397278, prediction : ihello\n",
      "epoch82 | loss : 0.5395171046257019, prediction : ihello\n",
      "epoch83 | loss : 0.5774552226066589, prediction : ilello\n",
      "epoch84 | loss : 0.5462673306465149, prediction : ilello\n",
      "epoch85 | loss : 0.5476320385932922, prediction : ilello\n",
      "epoch86 | loss : 0.5574381351470947, prediction : ilello\n",
      "epoch87 | loss : 0.5623838305473328, prediction : ilello\n",
      "epoch88 | loss : 0.5626820921897888, prediction : ilello\n",
      "epoch89 | loss : 0.5589341521263123, prediction : ilello\n",
      "epoch90 | loss : 0.5515211224555969, prediction : ilello\n",
      "epoch91 | loss : 0.5430334210395813, prediction : ilello\n",
      "epoch92 | loss : 0.5476207137107849, prediction : ilello\n",
      "epoch93 | loss : 0.5511610507965088, prediction : ihello\n",
      "epoch94 | loss : 0.5397499203681946, prediction : ihello\n",
      "epoch95 | loss : 0.5422388911247253, prediction : ihello\n",
      "epoch96 | loss : 0.5452605485916138, prediction : ihello\n",
      "epoch97 | loss : 0.5451850891113281, prediction : ihello\n",
      "epoch98 | loss : 0.5420438051223755, prediction : ihello\n",
      "epoch99 | loss : 0.5375363230705261, prediction : ihello\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, status = model(X)\n",
    "    \n",
    "    if i == 0:\n",
    "        print(outputs.shape, status.shape)\n",
    "    \n",
    "    loss = criterion(outputs.view(-1, input_dim), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
    "\n",
    "    print(f\"epoch{i} | loss : {loss.item()}, prediction : {result_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
