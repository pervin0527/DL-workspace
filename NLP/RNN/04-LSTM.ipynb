{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import urllib\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data download\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
    "urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "with open('./sarcasm.json') as f:\n",
    "    datas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(datas)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'my', 'name', 'is', 'minjun', 'kim', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize example\n",
    "## 문장을 token화한다.\n",
    "sample = \"Hello, my name is minjun kim.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<Unknown>', \"'\", 'to', 'of', 'the', 's', 'in', ',', 'for', 'a', 'on', '.', 'and', 'with', 'is', 'trump', 'new', 'man', 'from', 'at', 'you', 't', 'it', 'about', 'this', 'by', 'after', '?', 'be', 'that', 'how', 'out', 'he', 'as', 'up', 'not', 'what', 'can', 'are', 'your', 'his', 'who', 'just', 'has', 'will', 'more', 'all', 'one', 'into', 'report', 'i', 'why', 'have', 'area', 'woman', 'over', 'donald', 'u', 'says', 'day', 'obama', 'time', 'no', 'first', 'like', 'people', 'women', 'get', 'her', 'we', 'world', 'an', 'now', 'nation', 'house', 'life', 'off', 'clinton', 'they', 'make', 'still', 'than', 'was', 'my', 'white', 'back', 'down', 'if', 'when', 'family', 'could', 'she', 'their', 'do', 'before', 'americans', 'gop', 'most', 'way', '5', 'black', 'year', 'here', 'study', 'years', 'bill', 'should', 'would', 'him', 'president', 'best', 'so', 'america', 'police', 'only', 'watch', 'school', 'show', 'american', 'really', 'being', 'but', 'know', 'home', 'mom', 'things', 'death', 'during', 'good', 'finds', 'state', ')', 'love', '(', 'or', 'say', 'video', 'going', 'hillary', 'health', 'last', '!', 'parents', 'may', 'big', 'campaign', 'against', 'every', 'too', 'don', 'need', '3', 'child', 'kids', 'party', 'gets', 'these', '10', 'getting', 'little', 'some', 'right', 'change', 'take', 'work', 'dead', 'our', 'court', 'makes', 'there', 're', 'calls', 'other', 'news', 'doesn', 'john', '000', 'through', 'while', 'own', 'want', 'never', 'look', 'dad', '2', 'takes', 'where', 'guy', 'star', 'war', 'bush', 'gay', 'see', 'local', 'next', 'stop', 'even', 'go', '--', 'its', 'real', 'week', 'college', 'election', 'won', 'again', 'dog', 'office', 'plan', 'another', 'baby', 'us', 'got', 'thing', 'around', 'game', 'made', 'them', 'help', 'children', 'couple', 'wants', 'gun', 'debate', 'job', 'live', 'million', '4', 'actually', 'care', 'night', 'congress', 'north', 'finally', 'high', 'much', '6', 'been', 'ever', 'me', 'god', 'men', 'sex', 'son', 'under', 'money', 'national', 'teen', '7', 'season', 'bad', 'climate', 'friend', 'two', 'ways', 'better', 'give', 'media', 'top', 'city', 'everyone', 'senate', 'shows', 'students', 'had', 'let', 'paul', 'shooting', 'm', 'reveals', 'sexual', 'story', 'trying', 'facebook', 'history', 'making', 'york', 'announces', 'away', 'food', 'old', 'supreme', 'without', 'business', 'pope', 'attack', 'fight', 'mother', 'any', 'deal', 'enough', 'friends', 'girl', 'movie', 'tv', 'face', 'end', 'film', 'introduces', 'company', 'government', 'great', 'law', 'sanders', 'call', 'does', 'fire', 'part', 'tell', 'entire', 'former', '&', 'body', 'book', 'find', 'found', 'wedding', 'come', 'father', 'think', 'pretty', 'll', 'republican', '8', 'speech', 'support', 'coming', 'single', 'future', 'morning', 'photos', 'presidential', 'public', 'self', 'car', 'd', 'james', 'keep', 'republicans', 'run', 'use', 'power', 'thinks', 'very', 'already', 'christmas', 'didn', 'name', 'email', 'rights', 'talk', '2016', 'case', 'democrats', 'marriage', 'student', 'violence', 'behind', 'between', 'country', 'line', 'race', 'releases', 'boy', 'killed', 'ryan', 'tax', 'teacher', 'voters', 'doing', 'group', 'human', 'might', 'secret', 'security', 'something', 'vote', 'california', 'fans', 'goes', 'having', 'long', 'used', '1', 'must', 'today', 'asks', 'bernie', 'free', 'team', 'twitter', 'ban', 'department', 'wife', 'win', 'looking', 'open', 'poll', 'ad', 'because', 'girls', 'middle', 'room', 'biden', 'each', 'judge', 'obamacare', 'once', 'sure', 've', 'days', 'inside', 'minutes', 'political', 'art', 'claims', 'control', 'everything', 'forced', 'music', 'super', 'daughter', 'many', 'meet', 'missing', 'perfect', 'running', 'save', 'states', 'person', 'second', 'times', 'unveils', '20', 'candidate', 'ceo', 'living', 'plans', 'put', 'reports', 'same', 'scientists', 'social', 'summer', 'always', 'employee', 'justice', 'photo', 'tells', 'dies', 'red', 'until', 'did', 'female', 'full', 'korea', 'looks', 'talks', 'texas', 'were', 'comes', 'cruz', 'head', 'michael', 'needs', 'pay', 'ready', 'romney', 'russia', 'secretary', 'warns', '12', 'admits', 'cancer', 'kim', 'list', 'lives', 'mike', 'past', 'taking', 'town', 'wall', 'water', 'crisis', 'earth', 'gives', 'idea', 'record', 'set', 'working', 'wrong', 'administration', 'director', 'george', 'hot', 'letter', 'restaurant', 'shot', 'thought', 'heart', 'iran', 'left', 'someone', 'wins', 'dream', 'hours', 'meeting', 'probably', 'service', 'south', 'start', 'young', 'age', 'education', 'fan', 'fbi', 'hollywood', 'lost', 'nothing', 'percent', 'rock', 'street', 'tips', 'cat', 'china', 'isis', 'kill', 'king', 'owner', 'place', 'thousands', 'three', 'washington', 'believe', 'breaking', 'few', 'fucking', 'phone', 'together', 'tweets', 'chief', 'eating', 'feel', 'florida', 'francis', 'internet', 'march', 'officials', 'stars', 'talking', 'yet', 'attacks', 'chris', 'class', 'federal', 'guide', 'leaves', 'military', 'online', 'order', 'questions', 'ted', 'birthday', 'drug', 'move', 'air', 'ask', 'beautiful', 'congressman', 'giving', 'kid', 'latest', 'months', 'problem', 'reason', 'sleep', 'small', '9', 'assault', 'buy', 'different', 'happy', 'huffpost', 'less', 'majority', 'nuclear', 'personal', 'rules', 'series', 'word', '-', '2015', 'democratic', 'fox', 'girlfriend', 'holiday', 'isn', 'lot', 'moment', 'month', 'outside', 'prison', 'reasons', 'shit', 'system', 'those', 'told', 'community', 'favorite', 'hard', 'kind', 'leave', 'response', 'rise', 'travel', 'bar', 'fun', 'interview', 'issues', 'jimmy', 'mark', 'play', 'relationship', 'senator', 'box', 'celebrates', 'excited', 'huge', 'ice', 'immigration', 'leaders', 'read', 'scott', 'special', 'weekend', 'cop', 'cover', 'following', 'gift', 'hope', 'knows', 'message', 'o', 'offers', 'pence', 'politics', 'protest', 'stephen', 'syria', 'thinking', 'tom', 'trailer', 'trip', 'union', 'using', 'visit', 'watching', 'david', 'hate', 'hit', 'leader', 'lessons', 'muslim', 'russian', 'since', 'straight', 'taylor', 'victims', 'accused', 'anything', 'apple', 'bring', 'conversation', 'front', 'hair', 'himself', 'adorable', 'birth', 'break', 'candidates', 'chinese', 'cops', 'date', 'drunk', 'joe', 'least', 'millions', 'prince', 'queer', 'united', 'words', 'become', 'billion', 'fall', 'industry', 'investigation', 'learned', 'mass', 'opens', 'powerful', 'reality', 'rubio', 'third', 'totally', 'whole', 'career', 'die', 'early', 'employees', 'friday', 'iraq', 'returns', 'true', 'waiting', '11', '15', 'almost', 'amazon', 'artist', 'breaks', 'fashion', 'feels', 'kardashian', 'massive', 'point', 'policy', 'schools', 'stage', 'syrian', 'turn', 'wars', 'well', 'abortion', 'awards', 'cnn', 'completely', 'coworker', 'dance', 'dating', 'dinner', 'hits', 'jenner', 'jr', 'killing', 'kills', 'murder', 'puts', 'song', 'spends', 'sports', 'turns', 'wearing', 'west', 'workers', 'worried', '50', 'center', 'crash', 'experience', 'final', 'global', 'host', 'late', 'names', 'post', 'sick', 'sign', 'signs', 'vows', 'weird', 'worst', '30', 'adds', 'anniversary', 'called', 'chicago', 'decision', 'ferguson', 'fighting', 'governor', 'hands', 'j', 'key', 'moving', 'netflix', 'nfl', 'reportedly', 'seen', 'store', 'surprise', 'trans', 'transgender', 'university', '2014', '2017', 'across', 'act', 'advice', 'band', 'bus', 'c', 'discover', 'force', 'google', 'keeps', 'magazine', 'michelle', 'moms', 'official', 'paris', 'return', 'road', 'struggling', 'suicide', 'voice', '9/11', 'brings', 'executive', 'fear', 'harry', 'important', 'kerry', 'lead', 'light', 'longer', 'lose', 'nyc', 'peace', 'planned', 'reform', 'suspect', 'users', 'walking', 'apartment', 'audience', 'biggest', 'carolina', 'coffee', 'eat', 'football', 'given', 'hall', 'hoping', 'iowa', 'jobs', 'members', 'oil', 'oscars', 'program', 'question', 'stand', 'starting', 'steve', 'swift', 'test', 'wait', 'weight', 'whether', 'worth', 'beauty', 'chance', 'doctor', 'general', 'grandma', 'major', 'mind', 'planet', 'possible', 'protesters', 'remember', 'risk', 'success', 'which', 'abuse', 'allegations', 'clearly', 'colbert', 'cool', 'cut', 'demands', 'five', 'fuck', 'halloween', 'hand', 'hear', 'hurricane', 'ideas', 'israel', 'mental', 'moore', 'park', 'playing', 'poor', 'press', 'queen', 'role', 'space', 'teens', 'tour', 'try', 'williams', 'asking', 'ben', 'board', 'celebrate', 'dying', 'far', 'green', 'hero', 'hilarious', 'homeless', 'officer', 'oscar', 'plane', 'pregnant', 'problems', 'push', 'reporter', 'sean', 'side', 'supporters', 'table', 'throws', 'vacation', 'voter', 'website', 'wishes', 'album', 'amazing', 'begins', 'boss', 'building', 'card', 'chicken', 'church', 'data', 'deadly', 'economy', 'emotional', 'epa', 'eyes', 'fails', 'families', 'feeling', 'happened', 'humans', 'kelly', 'leads', 'learn', 'manager', 'marijuana', 'number', 'picture', 'pizza', 'proud', 'receives', 'refugees', 'rest', 'results', 'reveal', 'robert', 'shares', 'simple', 'snl', 'suggests', 'urges', 'uses', '13', 'address', 'amid', 'apparently', 'arrested', 'boyfriend', 'boys', 'check', 'close', 'comey', 'demand', 'desperate', 'door', 'driver', 'driving', 'easy', 'finding', 'hour', 'leaving', 'lgbt', 'male', 'older', 'performance', 'quietly', 'racist', 'rally', 'sales', 'shop', 'went', 'worse', 'actor', 'al', 'anyone']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 7, 83, 347, 14, 0, 474, 11]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yield_tokens(sentences):\n",
    "    for text in sentences:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['headline'].tolist()),\n",
    "                                  specials=[\"<Unknown>\"], ## 어휘에 없는 단어들을 \"<Unknown>\"로 대체\n",
    "                                  min_freq=2,\n",
    "                                  max_tokens=1000,)\n",
    "vocab.set_default_index(vocab['<Unknown>']) ## 생성된 어휘에서 \"<UNK>\" 토큰을 기본 인덱스로 설정\n",
    "\n",
    "str_to_idx = vocab.get_stoi()\n",
    "idx_to_str = vocab.get_itos()\n",
    "\n",
    "print(idx_to_str)\n",
    "vocab(tokenizer(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "84\n",
      "the 'roseanne' revival catches up to our thorny political mood, for better and worse\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[\"headline\"],\n",
    "                                                    df['is_sarcastic'],\n",
    "                                                    stratify=df['is_sarcastic'],\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "print(len(x_train[0]))\n",
    "print(x_train[0])\n",
    "\n",
    "print(len(x_train[1]))\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        return self.vocab(self.tokenizer(text)), label\n",
    "    \n",
    "def collate_batch(batch, max_sequence_length):\n",
    "    label_list, text_list = [], []\n",
    "\n",
    "    for text, label in batch:\n",
    "        processed_text = torch.tensor(text[:max_sequence_length], dtype=torch.int64) ## 길이를 max_sequence_length를 넘지 못하게 만든다.\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0) ## padding을 통해 데이터의 길이를 일정하게 맞춰준다.\n",
    "\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([96, 568, 415, 0, 43, 45, 0, 664, 139, 229, 829], 0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train, vocab=vocab, tokenizer=tokenizer)\n",
    "valid_dataset = CustomDataset(x_test, y_test, vocab=vocab, tokenizer=tokenizer)\n",
    "\n",
    "for data in train_dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 22]) torch.Size([32])\n",
      "tensor([ 17, 926,  66,   6,   0, 615, 107, 265, 108, 631,   0,   0,   0,   6,\n",
      "        827,   0,   0,   0,   0,   0,   0,   0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x : collate_batch(x, MAX_SEQUENCE_LEN)) ## 최대 길이가 100\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x : collate_batch(x, MAX_SEQUENCE_LEN))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    tokens, labels = data[0], data[1]\n",
    "    print(tokens.shape, labels.shape)\n",
    "    print(tokens[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCAB = len(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1000, 30)\n",
      "torch.Size([32, 20, 30])\n",
      "tensor([[ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [-4.5078e-01,  3.2779e-01, -1.8165e+00, -1.4904e+00, -1.3346e+00,\n",
      "         -6.2459e-01, -2.1514e-02,  1.2991e+00,  8.5966e-01,  5.7346e-01,\n",
      "          6.8901e-01, -1.8121e+00, -1.5993e-01, -9.5905e-01, -1.9809e+00,\n",
      "          1.5688e+00,  8.5423e-02, -9.6092e-01,  3.5714e-01, -2.0792e+00,\n",
      "         -8.7392e-01,  7.5145e-01,  3.5993e-02, -1.9472e+00,  2.6197e+00,\n",
      "          1.0305e+00,  2.6651e-01,  6.7245e-01, -6.7598e-01,  4.0065e-01],\n",
      "        [ 7.8453e-01, -6.6049e-01,  4.4225e-01,  1.6733e+00,  4.9676e-01,\n",
      "         -2.3119e+00,  7.5211e-01, -8.5825e-02, -6.7463e-01,  1.1483e+00,\n",
      "         -8.2365e-01, -1.3196e+00, -1.9891e+00,  6.0618e-01,  6.2785e-01,\n",
      "         -8.3545e-01,  8.4194e-01, -7.7413e-01, -1.8234e-01, -4.1428e-01,\n",
      "         -6.5576e-01, -1.7685e+00, -1.5412e-01,  5.2335e-02,  7.4005e-01,\n",
      "          2.0474e+00, -5.3231e-01,  1.8405e+00, -1.1257e+00, -7.7039e-01],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [-2.7496e+00, -7.7582e-01, -5.7596e-01,  1.1430e+00,  3.3750e-01,\n",
      "         -2.4687e-01,  5.6541e-01, -2.5114e-01, -1.3670e+00,  7.3555e-01,\n",
      "          1.3557e+00,  2.0590e-01, -9.6101e-01, -6.9246e-01,  8.4438e-01,\n",
      "          1.1268e+00, -1.0560e+00,  1.0150e+00, -4.8225e-02, -5.5313e-01,\n",
      "         -3.1549e-01,  5.2285e-01, -1.4221e+00, -2.4272e-01, -1.5247e+00,\n",
      "         -4.2688e-01, -1.0547e-01, -6.3912e-01, -1.0059e-01, -1.9143e-02],\n",
      "        [ 2.1186e-01, -7.3283e-01,  2.7183e+00, -4.5067e-01, -1.2598e+00,\n",
      "          7.2258e-01, -1.9443e+00,  7.7485e-01,  1.0506e+00, -1.5193e+00,\n",
      "         -1.4410e+00, -1.2253e+00, -8.1514e-01, -2.2595e-02,  1.8614e+00,\n",
      "          1.6087e+00, -2.6207e-02,  3.1692e-02,  1.2269e+00,  4.4421e-02,\n",
      "         -1.4491e+00, -3.0420e-01, -3.9928e-01,  3.5226e-01,  7.5588e-01,\n",
      "          4.1928e-01, -8.6889e-01, -6.7326e-01,  9.6848e-01, -7.2521e-01],\n",
      "        [-4.5078e-01,  3.2779e-01, -1.8165e+00, -1.4904e+00, -1.3346e+00,\n",
      "         -6.2459e-01, -2.1514e-02,  1.2991e+00,  8.5966e-01,  5.7346e-01,\n",
      "          6.8901e-01, -1.8121e+00, -1.5993e-01, -9.5905e-01, -1.9809e+00,\n",
      "          1.5688e+00,  8.5423e-02, -9.6092e-01,  3.5714e-01, -2.0792e+00,\n",
      "         -8.7392e-01,  7.5145e-01,  3.5993e-02, -1.9472e+00,  2.6197e+00,\n",
      "          1.0305e+00,  2.6651e-01,  6.7245e-01, -6.7598e-01,  4.0065e-01],\n",
      "        [ 7.8453e-01, -6.6049e-01,  4.4225e-01,  1.6733e+00,  4.9676e-01,\n",
      "         -2.3119e+00,  7.5211e-01, -8.5825e-02, -6.7463e-01,  1.1483e+00,\n",
      "         -8.2365e-01, -1.3196e+00, -1.9891e+00,  6.0618e-01,  6.2785e-01,\n",
      "         -8.3545e-01,  8.4194e-01, -7.7413e-01, -1.8234e-01, -4.1428e-01,\n",
      "         -6.5576e-01, -1.7685e+00, -1.5412e-01,  5.2335e-02,  7.4005e-01,\n",
      "          2.0474e+00, -5.3231e-01,  1.8405e+00, -1.1257e+00, -7.7039e-01],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [ 8.7708e-01, -4.7068e-01, -3.5991e-01,  2.8979e-01, -1.7030e+00,\n",
      "          6.2988e-01,  8.0462e-01, -7.3876e-01, -6.1158e-01, -1.7632e+00,\n",
      "         -4.2526e-01,  1.4446e+00, -6.4033e-01, -9.9698e-02, -5.9373e-01,\n",
      "          1.6480e-01,  7.5998e-01,  1.9840e+00, -4.8814e-02, -9.4722e-01,\n",
      "          9.9331e-01,  4.5580e-01, -1.0923e+00,  7.3733e-01,  3.0983e-01,\n",
      "          9.8048e-02, -1.6540e+00,  2.2082e-01, -8.2785e-01,  4.9722e-01],\n",
      "        [ 1.2410e+00,  1.2013e+00, -9.3162e-02,  1.8087e+00,  2.2325e+00,\n",
      "         -1.3614e+00, -1.6720e+00,  1.3266e+00, -2.4395e-01, -1.2111e+00,\n",
      "         -1.8095e+00, -2.7539e-01, -4.9090e-01, -1.4080e+00,  1.8332e+00,\n",
      "          6.1838e-03,  1.6552e+00,  1.2247e+00,  2.2742e+00,  1.2713e+00,\n",
      "          1.3046e+00,  5.3178e-01, -1.2634e+00,  2.7192e-01, -8.8861e-01,\n",
      "         -3.8262e-01, -1.1725e+00,  1.1131e+00,  5.9891e-01, -1.4309e+00],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [-4.5078e-01,  3.2779e-01, -1.8165e+00, -1.4904e+00, -1.3346e+00,\n",
      "         -6.2459e-01, -2.1514e-02,  1.2991e+00,  8.5966e-01,  5.7346e-01,\n",
      "          6.8901e-01, -1.8121e+00, -1.5993e-01, -9.5905e-01, -1.9809e+00,\n",
      "          1.5688e+00,  8.5423e-02, -9.6092e-01,  3.5714e-01, -2.0792e+00,\n",
      "         -8.7392e-01,  7.5145e-01,  3.5993e-02, -1.9472e+00,  2.6197e+00,\n",
      "          1.0305e+00,  2.6651e-01,  6.7245e-01, -6.7598e-01,  4.0065e-01],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [ 1.0588e+00, -1.9556e-01, -1.4365e+00, -3.9843e-01, -8.7625e-01,\n",
      "          1.3841e-01,  3.8936e-01,  1.9782e+00, -1.0303e+00,  1.7418e-01,\n",
      "          1.2488e-01, -6.2348e-01, -5.8052e-01, -5.3591e-01,  1.1718e-02,\n",
      "         -3.8238e-01,  1.3032e-01, -8.2368e-01,  1.5924e-01, -3.5247e-01,\n",
      "          3.0118e-01, -8.0127e-02,  1.9685e+00,  2.7724e-02, -5.8897e-02,\n",
      "          1.2683e+00, -6.7063e-01, -7.4522e-02,  1.0198e+00,  8.0853e-01],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [ 1.1318e+00,  5.8270e-01,  4.5698e-01, -6.7640e-01, -4.5282e-01,\n",
      "         -3.9424e-01, -2.8328e-01, -3.0695e-01,  2.8230e-01, -6.3339e-01,\n",
      "         -2.4154e+00,  2.2593e-01,  6.9205e-01, -7.6513e-01,  5.2927e-01,\n",
      "         -5.3513e-01,  7.4964e-01, -6.0023e-01, -4.2517e-01,  2.9958e-01,\n",
      "         -1.5050e+00, -5.9104e-01, -6.1813e-01, -1.5524e+00, -3.7867e-01,\n",
      "         -9.9839e-01, -2.9230e-02,  5.4409e-01, -7.1100e-01, -1.3992e+00],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00],\n",
      "        [ 5.6780e-01, -1.8588e-01, -1.0390e+00, -1.5141e+00,  4.4408e-01,\n",
      "         -1.3901e-01, -9.3228e-01, -1.7407e+00, -1.9198e+00, -1.3429e+00,\n",
      "          4.8739e-01, -1.0344e+00,  1.9785e+00, -9.5842e-02, -5.3842e-01,\n",
      "         -2.1118e-03,  3.5626e-01, -1.4527e+00, -1.8026e-01,  8.5381e-01,\n",
      "          8.5727e-01, -7.3970e-01, -5.4882e-01,  8.9367e-01,  5.1219e-01,\n",
      "          7.1578e-01,  6.1685e-01,  1.9030e-01,  1.4914e+00, -1.3061e+00]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "embedding = nn.Embedding(len(vocab), EMBEDDING_DIM).to(device) ## len(vocab)개의 단어들을 EMBEDDING_DIM 크기의 실수 벡터로 변환하는 임베딩을 생성.\n",
    "print(embedding)\n",
    "\n",
    "embedding_out = embedding(x)\n",
    "print(embedding_out.shape)\n",
    "print(embedding_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(30, 64, batch_first=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = 1\n",
    "SEQ_LENGTH = x.size(1)\n",
    "\n",
    "lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, batch_first=True, device=device)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)\n",
    "c_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 64])\n",
      "torch.Size([1, 32, 64]) torch.Size([1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "lstm_out, (hidden, cell) = lstm(embedding_out)\n",
    "print(lstm_out.shape)\n",
    "print(hidden.shape, cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingLSTM(x, vocab_size, embedding_dim, hidden_size, bidrectional, num_layers, device):\n",
    "    print(f\"Input Shape : {x.shape}\")\n",
    "    x = x.to(device)\n",
    "    \n",
    "    \n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
    "    embedding_out = embedding(x)\n",
    "    print(f\"Embedded Input Shape : {embedding_out.shape}\")\n",
    "\n",
    "    lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                   hidden_size=hidden_size,\n",
    "                   num_layers=num_layers,\n",
    "                   batch_first=True,\n",
    "                   device=device)\n",
    "    \n",
    "    bidi = 2 if bidrectional else 1\n",
    "    out, (h, c) = lstm(embedding_out)\n",
    "    print(f\"Output Shape : {out.shape}\")\n",
    "    print(f\"Hidden Shape : {h.shape}\")\n",
    "    print(f\"Cell State Shape : {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape : torch.Size([32, 20])\n",
      "Embedded Input Shape : torch.Size([32, 20, 30])\n",
      "Output Shape : torch.Size([32, 20, 64])\n",
      "Hidden Shape : torch.Size([2, 32, 64])\n",
      "Cell State Shape : torch.Size([2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "EmbeddingLSTM(x, vocab_size=len(vocab), embedding_dim=30, hidden_size=64, bidrectional=False, num_layers=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, hidden_size, num_layers, bidirectional=True, drop_prob=0.1):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                           )\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def init_hidden_and_cell_state(self, batch_size, device):\n",
    "        # LSTM 입력시 초기 Cell 에 대한 가중치 초기화를 진행합니다.\n",
    "        # (num_layers*bidirectional, batch_size, hidden_size)\n",
    "        self.hidden_and_cell = (\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x, self.hidden_and_cell)\n",
    "        # (batch_size, seq_length, hidden_size*bidirectional)\n",
    "        # last sequence 의 (batch_size, hidden_size*bidirectional)\n",
    "        h = output[:, -1, :]\n",
    "        o = self.dropout(h)\n",
    "        o = self.relu(self.fc(o))\n",
    "        o = self.dropout(o)\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): Embedding(1000, 16)\n",
       "  (lstm): LSTM(16, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'num_classes': 2, \n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 16, \n",
    "    'hidden_size': 32, \n",
    "    'num_layers': 2, \n",
    "    'bidirectional': True,\n",
    "}\n",
    "\n",
    "model = TextClassificationModel(**config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 정의: CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    for idx, (txt, lbl) in enumerate(prograss_bar):\n",
    "        txt = txt.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.init_hidden_and_cell_state(len(txt), device)\n",
    "        \n",
    "        output = model(txt)\n",
    "        \n",
    "        loss = loss_fn(output, lbl)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        output = output.argmax(dim=1)\n",
    "        \n",
    "        corr += (output == lbl).sum().item()\n",
    "        counts += len(lbl)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "    \n",
    "    acc = corr / len(data_loader.dataset)\n",
    "\n",
    "    return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        for txt, lbl in data_loader:\n",
    "            txt = txt.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "            model.init_hidden_and_cell_state(len(txt), device)\n",
    "    \n",
    "            output = model(txt)\n",
    "            loss = loss_fn(output, lbl)\n",
    "            output = output.argmax(dim=1)\n",
    "\n",
    "            corr += (output == lbl).sum().item()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.51338, training accuracy: 0.74840: 100%|██████████| 668/668 [00:01<00:00, 469.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.44937. Saving Model!\n",
      "epoch 01, loss: 0.51338, acc: 0.74840, val_loss: 0.44937, val_accuracy: 0.78940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.44191, training accuracy: 0.79506: 100%|██████████| 668/668 [00:01<00:00, 459.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.44937 to 0.42500. Saving Model!\n",
      "epoch 02, loss: 0.44191, acc: 0.79506, val_loss: 0.42500, val_accuracy: 0.80513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.40298, training accuracy: 0.81556: 100%|██████████| 668/668 [00:01<00:00, 470.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.42500 to 0.38990. Saving Model!\n",
      "epoch 03, loss: 0.40298, acc: 0.81556, val_loss: 0.38990, val_accuracy: 0.82497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.37599, training accuracy: 0.83002: 100%|██████████| 668/668 [00:01<00:00, 512.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.38990 to 0.37807. Saving Model!\n",
      "epoch 04, loss: 0.37599, acc: 0.83002, val_loss: 0.37807, val_accuracy: 0.83265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.35653, training accuracy: 0.84097: 100%|██████████| 668/668 [00:01<00:00, 470.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.37807 to 0.36324. Saving Model!\n",
      "epoch 05, loss: 0.35653, acc: 0.84097, val_loss: 0.36324, val_accuracy: 0.83602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.33982, training accuracy: 0.84902: 100%|██████████| 668/668 [00:01<00:00, 458.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.36324 to 0.35874. Saving Model!\n",
      "epoch 06, loss: 0.33982, acc: 0.84902, val_loss: 0.35874, val_accuracy: 0.83901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.32610, training accuracy: 0.85426: 100%|██████████| 668/668 [00:01<00:00, 466.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07, loss: 0.32610, acc: 0.85426, val_loss: 0.36131, val_accuracy: 0.84051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.31487, training accuracy: 0.85955: 100%|██████████| 668/668 [00:01<00:00, 463.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08, loss: 0.31487, acc: 0.85955, val_loss: 0.37519, val_accuracy: 0.84425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.30416, training accuracy: 0.86643: 100%|██████████| 668/668 [00:01<00:00, 469.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09, loss: 0.30416, acc: 0.86643, val_loss: 0.36255, val_accuracy: 0.84388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.29213, training accuracy: 0.87279: 100%|██████████| 668/668 [00:01<00:00, 474.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.29213, acc: 0.87279, val_loss: 0.38074, val_accuracy: 0.83021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.28369, training accuracy: 0.87607: 100%|██████████| 668/668 [00:01<00:00, 475.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.28369, acc: 0.87607, val_loss: 0.36825, val_accuracy: 0.84238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.27158, training accuracy: 0.87930: 100%|██████████| 668/668 [00:01<00:00, 494.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 0.27158, acc: 0.87930, val_loss: 0.38200, val_accuracy: 0.83714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.26138, training accuracy: 0.88492: 100%|██████████| 668/668 [00:01<00:00, 485.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 0.26138, acc: 0.88492, val_loss: 0.37929, val_accuracy: 0.83677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.24944, training accuracy: 0.88988: 100%|██████████| 668/668 [00:01<00:00, 525.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 0.24944, acc: 0.88988, val_loss: 0.39755, val_accuracy: 0.84145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.23973, training accuracy: 0.89811: 100%|██████████| 668/668 [00:01<00:00, 510.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 0.23973, acc: 0.89811, val_loss: 0.40683, val_accuracy: 0.83976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.22721, training accuracy: 0.90293: 100%|██████████| 668/668 [00:01<00:00, 473.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 0.22721, acc: 0.90293, val_loss: 0.43756, val_accuracy: 0.83920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.21675, training accuracy: 0.90710: 100%|██████████| 668/668 [00:01<00:00, 475.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 0.21675, acc: 0.90710, val_loss: 0.41442, val_accuracy: 0.83826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.21023, training accuracy: 0.90958: 100%|██████████| 668/668 [00:01<00:00, 486.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 0.21023, acc: 0.90958, val_loss: 0.43847, val_accuracy: 0.83789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.20320, training accuracy: 0.91412: 100%|██████████| 668/668 [00:01<00:00, 472.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 0.20320, acc: 0.91412, val_loss: 0.46468, val_accuracy: 0.83714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19123, training accuracy: 0.91852: 100%|██████████| 668/668 [00:01<00:00, 475.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss: 0.19123, acc: 0.91852, val_loss: 0.44767, val_accuracy: 0.83920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.18419, training accuracy: 0.92044: 100%|██████████| 668/668 [00:01<00:00, 488.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, loss: 0.18419, acc: 0.92044, val_loss: 0.47779, val_accuracy: 0.83826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.17625, training accuracy: 0.92582: 100%|██████████| 668/668 [00:01<00:00, 489.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss: 0.17625, acc: 0.92582, val_loss: 0.53261, val_accuracy: 0.83620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.16986, training accuracy: 0.92928: 100%|██████████| 668/668 [00:01<00:00, 492.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23, loss: 0.16986, acc: 0.92928, val_loss: 0.53854, val_accuracy: 0.82984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.16016, training accuracy: 0.93265: 100%|██████████| 668/668 [00:01<00:00, 485.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, loss: 0.16016, acc: 0.93265, val_loss: 0.55317, val_accuracy: 0.83396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15746, training accuracy: 0.93331: 100%|██████████| 668/668 [00:01<00:00, 460.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25, loss: 0.15746, acc: 0.93331, val_loss: 0.55691, val_accuracy: 0.83265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.14572, training accuracy: 0.93958: 100%|██████████| 668/668 [00:01<00:00, 471.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26, loss: 0.14572, acc: 0.93958, val_loss: 0.63120, val_accuracy: 0.83096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.14465, training accuracy: 0.93897: 100%|██████████| 668/668 [00:01<00:00, 461.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, loss: 0.14465, acc: 0.93897, val_loss: 0.56265, val_accuracy: 0.83152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.13737, training accuracy: 0.94389: 100%|██████████| 668/668 [00:01<00:00, 457.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28, loss: 0.13737, acc: 0.94389, val_loss: 0.56743, val_accuracy: 0.83059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.13118, training accuracy: 0.94604: 100%|██████████| 668/668 [00:01<00:00, 472.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, loss: 0.13118, acc: 0.94604, val_loss: 0.67143, val_accuracy: 0.82946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.12759, training accuracy: 0.94716: 100%|██████████| 668/668 [00:01<00:00, 484.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, loss: 0.12759, acc: 0.94716, val_loss: 0.63170, val_accuracy: 0.83265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.12434, training accuracy: 0.94828: 100%|██████████| 668/668 [00:01<00:00, 465.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, loss: 0.12434, acc: 0.94828, val_loss: 0.65236, val_accuracy: 0.83040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11533, training accuracy: 0.95179: 100%|██████████| 668/668 [00:01<00:00, 464.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32, loss: 0.11533, acc: 0.95179, val_loss: 0.65434, val_accuracy: 0.82909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11723, training accuracy: 0.95161: 100%|██████████| 668/668 [00:01<00:00, 485.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, loss: 0.11723, acc: 0.95161, val_loss: 0.69722, val_accuracy: 0.82909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11166, training accuracy: 0.95470: 100%|██████████| 668/668 [00:01<00:00, 481.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, loss: 0.11166, acc: 0.95470, val_loss: 0.70454, val_accuracy: 0.83246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.10915, training accuracy: 0.95554: 100%|██████████| 668/668 [00:01<00:00, 471.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, loss: 0.10915, acc: 0.95554, val_loss: 0.68944, val_accuracy: 0.83134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.10246, training accuracy: 0.95671: 100%|██████████| 668/668 [00:01<00:00, 473.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, loss: 0.10246, acc: 0.95671, val_loss: 0.66513, val_accuracy: 0.83040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.10184, training accuracy: 0.95802: 100%|██████████| 668/668 [00:01<00:00, 470.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37, loss: 0.10184, acc: 0.95802, val_loss: 0.71646, val_accuracy: 0.83152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09749, training accuracy: 0.96083: 100%|██████████| 668/668 [00:01<00:00, 475.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38, loss: 0.09749, acc: 0.96083, val_loss: 0.78535, val_accuracy: 0.82591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09487, training accuracy: 0.96069: 100%|██████████| 668/668 [00:01<00:00, 492.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39, loss: 0.09487, acc: 0.96069, val_loss: 0.75646, val_accuracy: 0.82254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09006, training accuracy: 0.96279: 100%|██████████| 668/668 [00:01<00:00, 472.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, loss: 0.09006, acc: 0.96279, val_loss: 0.76735, val_accuracy: 0.83059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09250, training accuracy: 0.96148: 100%|██████████| 668/668 [00:01<00:00, 476.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, loss: 0.09250, acc: 0.96148, val_loss: 0.69540, val_accuracy: 0.82909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.08379, training accuracy: 0.96593: 100%|██████████| 668/668 [00:01<00:00, 485.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42, loss: 0.08379, acc: 0.96593, val_loss: 0.78143, val_accuracy: 0.82516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09006, training accuracy: 0.96218: 100%|██████████| 668/668 [00:01<00:00, 529.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43, loss: 0.09006, acc: 0.96218, val_loss: 0.78868, val_accuracy: 0.82834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.08288, training accuracy: 0.96481: 100%|██████████| 668/668 [00:01<00:00, 480.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44, loss: 0.08288, acc: 0.96481, val_loss: 0.90844, val_accuracy: 0.82216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07748, training accuracy: 0.96785: 100%|██████████| 668/668 [00:01<00:00, 477.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45, loss: 0.07748, acc: 0.96785, val_loss: 0.90688, val_accuracy: 0.82666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07012, training accuracy: 0.96991: 100%|██████████| 668/668 [00:01<00:00, 490.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, loss: 0.07012, acc: 0.96991, val_loss: 0.83690, val_accuracy: 0.82741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07225, training accuracy: 0.97005: 100%|██████████| 668/668 [00:01<00:00, 478.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, loss: 0.07225, acc: 0.97005, val_loss: 0.86883, val_accuracy: 0.82872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07206, training accuracy: 0.96902: 100%|██████████| 668/668 [00:01<00:00, 494.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, loss: 0.07206, acc: 0.96902, val_loss: 0.95939, val_accuracy: 0.82778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07102, training accuracy: 0.97164: 100%|██████████| 668/668 [00:01<00:00, 481.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, loss: 0.07102, acc: 0.97164, val_loss: 0.88788, val_accuracy: 0.82273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06544, training accuracy: 0.97234: 100%|██████████| 668/668 [00:01<00:00, 495.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 0.06544, acc: 0.97234, val_loss: 1.02424, val_accuracy: 0.82647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06544, training accuracy: 0.97150: 100%|██████████| 668/668 [00:01<00:00, 487.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51, loss: 0.06544, acc: 0.97150, val_loss: 0.92731, val_accuracy: 0.82478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06916, training accuracy: 0.97183: 100%|██████████| 668/668 [00:01<00:00, 480.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, loss: 0.06916, acc: 0.97183, val_loss: 0.92492, val_accuracy: 0.80850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06927, training accuracy: 0.97159: 100%|██████████| 668/668 [00:01<00:00, 483.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, loss: 0.06927, acc: 0.97159, val_loss: 1.05229, val_accuracy: 0.82179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06134, training accuracy: 0.97487: 100%|██████████| 668/668 [00:01<00:00, 476.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, loss: 0.06134, acc: 0.97487, val_loss: 0.99010, val_accuracy: 0.82010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05590, training accuracy: 0.97613: 100%|██████████| 668/668 [00:01<00:00, 478.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55, loss: 0.05590, acc: 0.97613, val_loss: 1.09195, val_accuracy: 0.82628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06379, training accuracy: 0.97398: 100%|██████████| 668/668 [00:01<00:00, 481.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, loss: 0.06379, acc: 0.97398, val_loss: 0.84133, val_accuracy: 0.82067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05746, training accuracy: 0.97618: 100%|██████████| 668/668 [00:01<00:00, 492.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57, loss: 0.05746, acc: 0.97618, val_loss: 1.03810, val_accuracy: 0.82067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05821, training accuracy: 0.97580: 100%|██████████| 668/668 [00:01<00:00, 465.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58, loss: 0.05821, acc: 0.97580, val_loss: 0.97127, val_accuracy: 0.81917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05367, training accuracy: 0.97768: 100%|██████████| 668/668 [00:01<00:00, 458.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59, loss: 0.05367, acc: 0.97768, val_loss: 1.03248, val_accuracy: 0.82310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05294, training accuracy: 0.97697: 100%|██████████| 668/668 [00:01<00:00, 466.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60, loss: 0.05294, acc: 0.97697, val_loss: 1.04257, val_accuracy: 0.82067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05214, training accuracy: 0.97754: 100%|██████████| 668/668 [00:01<00:00, 456.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61, loss: 0.05214, acc: 0.97754, val_loss: 1.03970, val_accuracy: 0.82048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05544, training accuracy: 0.97683: 100%|██████████| 668/668 [00:01<00:00, 472.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62, loss: 0.05544, acc: 0.97683, val_loss: 1.08753, val_accuracy: 0.81037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05283, training accuracy: 0.97777: 100%|██████████| 668/668 [00:01<00:00, 461.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63, loss: 0.05283, acc: 0.97777, val_loss: 0.96844, val_accuracy: 0.82366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04600, training accuracy: 0.98020: 100%|██████████| 668/668 [00:01<00:00, 453.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64, loss: 0.04600, acc: 0.98020, val_loss: 1.17520, val_accuracy: 0.81992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05285, training accuracy: 0.97828: 100%|██████████| 668/668 [00:01<00:00, 457.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65, loss: 0.05285, acc: 0.97828, val_loss: 1.02435, val_accuracy: 0.82273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04718, training accuracy: 0.97988: 100%|██████████| 668/668 [00:01<00:00, 472.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66, loss: 0.04718, acc: 0.97988, val_loss: 1.04197, val_accuracy: 0.81674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04423, training accuracy: 0.98119: 100%|██████████| 668/668 [00:01<00:00, 466.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67, loss: 0.04423, acc: 0.98119, val_loss: 1.00192, val_accuracy: 0.81823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04308, training accuracy: 0.98062: 100%|██████████| 668/668 [00:01<00:00, 468.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68, loss: 0.04308, acc: 0.98062, val_loss: 1.27752, val_accuracy: 0.82797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04912, training accuracy: 0.98030: 100%|██████████| 668/668 [00:01<00:00, 468.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, loss: 0.04912, acc: 0.98030, val_loss: 1.08411, val_accuracy: 0.82048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04443, training accuracy: 0.98067: 100%|██████████| 668/668 [00:01<00:00, 488.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70, loss: 0.04443, acc: 0.98067, val_loss: 1.19051, val_accuracy: 0.81954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04514, training accuracy: 0.98109: 100%|██████████| 668/668 [00:01<00:00, 470.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71, loss: 0.04514, acc: 0.98109, val_loss: 1.19403, val_accuracy: 0.82216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03810, training accuracy: 0.98315: 100%|██████████| 668/668 [00:01<00:00, 476.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, loss: 0.03810, acc: 0.98315, val_loss: 1.21050, val_accuracy: 0.81973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04456, training accuracy: 0.98170: 100%|██████████| 668/668 [00:01<00:00, 480.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73, loss: 0.04456, acc: 0.98170, val_loss: 1.09296, val_accuracy: 0.82235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03984, training accuracy: 0.98282: 100%|██████████| 668/668 [00:01<00:00, 492.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74, loss: 0.03984, acc: 0.98282, val_loss: 1.12271, val_accuracy: 0.82422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04254, training accuracy: 0.98250: 100%|██████████| 668/668 [00:01<00:00, 489.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75, loss: 0.04254, acc: 0.98250, val_loss: 1.10979, val_accuracy: 0.82216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04057, training accuracy: 0.98301: 100%|██████████| 668/668 [00:01<00:00, 479.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76, loss: 0.04057, acc: 0.98301, val_loss: 0.95434, val_accuracy: 0.82067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03818, training accuracy: 0.98371: 100%|██████████| 668/668 [00:01<00:00, 468.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77, loss: 0.03818, acc: 0.98371, val_loss: 1.19322, val_accuracy: 0.82516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03286, training accuracy: 0.98559: 100%|██████████| 668/668 [00:01<00:00, 465.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78, loss: 0.03286, acc: 0.98559, val_loss: 1.28870, val_accuracy: 0.82460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03341, training accuracy: 0.98573: 100%|██████████| 668/668 [00:01<00:00, 512.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79, loss: 0.03341, acc: 0.98573, val_loss: 1.19445, val_accuracy: 0.82235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04270, training accuracy: 0.98306: 100%|██████████| 668/668 [00:01<00:00, 502.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80, loss: 0.04270, acc: 0.98306, val_loss: 1.23823, val_accuracy: 0.82310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04175, training accuracy: 0.98343: 100%|██████████| 668/668 [00:01<00:00, 480.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81, loss: 0.04175, acc: 0.98343, val_loss: 0.95974, val_accuracy: 0.82759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03234, training accuracy: 0.98690: 100%|██████████| 668/668 [00:01<00:00, 479.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82, loss: 0.03234, acc: 0.98690, val_loss: 1.22415, val_accuracy: 0.82123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03928, training accuracy: 0.98502: 100%|██████████| 668/668 [00:01<00:00, 464.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83, loss: 0.03928, acc: 0.98502, val_loss: 1.06197, val_accuracy: 0.82778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03471, training accuracy: 0.98605: 100%|██████████| 668/668 [00:01<00:00, 457.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84, loss: 0.03471, acc: 0.98605, val_loss: 1.11476, val_accuracy: 0.82366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03614, training accuracy: 0.98573: 100%|██████████| 668/668 [00:01<00:00, 467.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85, loss: 0.03614, acc: 0.98573, val_loss: 1.09008, val_accuracy: 0.82104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02888, training accuracy: 0.98769: 100%|██████████| 668/668 [00:01<00:00, 489.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86, loss: 0.02888, acc: 0.98769, val_loss: 1.25917, val_accuracy: 0.82535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03348, training accuracy: 0.98563: 100%|██████████| 668/668 [00:01<00:00, 495.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87, loss: 0.03348, acc: 0.98563, val_loss: 1.11567, val_accuracy: 0.81805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03389, training accuracy: 0.98596: 100%|██████████| 668/668 [00:01<00:00, 528.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88, loss: 0.03389, acc: 0.98596, val_loss: 1.03422, val_accuracy: 0.82179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03353, training accuracy: 0.98643: 100%|██████████| 668/668 [00:01<00:00, 491.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89, loss: 0.03353, acc: 0.98643, val_loss: 1.04326, val_accuracy: 0.81805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03152, training accuracy: 0.98694: 100%|██████████| 668/668 [00:01<00:00, 465.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90, loss: 0.03152, acc: 0.98694, val_loss: 1.07751, val_accuracy: 0.81954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02991, training accuracy: 0.98713: 100%|██████████| 668/668 [00:01<00:00, 459.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91, loss: 0.02991, acc: 0.98713, val_loss: 1.20988, val_accuracy: 0.82460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03034, training accuracy: 0.98666: 100%|██████████| 668/668 [00:01<00:00, 454.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92, loss: 0.03034, acc: 0.98666, val_loss: 1.45751, val_accuracy: 0.82198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02530, training accuracy: 0.98942: 100%|██████████| 668/668 [00:01<00:00, 460.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93, loss: 0.02530, acc: 0.98942, val_loss: 1.36401, val_accuracy: 0.82198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02968, training accuracy: 0.98783: 100%|██████████| 668/668 [00:01<00:00, 466.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94, loss: 0.02968, acc: 0.98783, val_loss: 1.20617, val_accuracy: 0.82010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03434, training accuracy: 0.98652: 100%|██████████| 668/668 [00:01<00:00, 458.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95, loss: 0.03434, acc: 0.98652, val_loss: 1.12470, val_accuracy: 0.82478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02700, training accuracy: 0.98895: 100%|██████████| 668/668 [00:01<00:00, 455.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96, loss: 0.02700, acc: 0.98895, val_loss: 1.32643, val_accuracy: 0.82553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02820, training accuracy: 0.98778: 100%|██████████| 668/668 [00:01<00:00, 458.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97, loss: 0.02820, acc: 0.98778, val_loss: 1.12741, val_accuracy: 0.82029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03026, training accuracy: 0.98760: 100%|██████████| 668/668 [00:01<00:00, 472.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98, loss: 0.03026, acc: 0.98760, val_loss: 1.19646, val_accuracy: 0.82123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02765, training accuracy: 0.98867: 100%|██████████| 668/668 [00:01<00:00, 480.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss: 0.02765, acc: 0.98867, val_loss: 1.21770, val_accuracy: 0.82404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02795, training accuracy: 0.98839: 100%|██████████| 668/668 [00:01<00:00, 452.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss: 0.02795, acc: 0.98839, val_loss: 1.30863, val_accuracy: 0.81954\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "model_name = 'LSTM-Text-Classification'\n",
    "\n",
    "min_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, valid_dataloader, loss_fn, device)   \n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "\n",
    "        try:\n",
    "            torch.save(model.state_dict(), f'/home/pervinco/Models/LSTM/{model_name}.pth')\n",
    "        except:\n",
    "            os.makedirs('/home/pervinco/Models/LSTM')\n",
    "    \n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
