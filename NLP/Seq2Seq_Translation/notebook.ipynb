{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/pervinco/DL-workspace/NLP/Seq2Seq_Translation\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from konlpy.tag import Mecab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from data.utils import get_total_data, tokenize, build_vocab, tokens_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/KORENG\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_data.csv exist.\n"
     ]
    }
   ],
   "source": [
    "df = get_total_data(data_dir)\n",
    "\n",
    "en_tokenizer = get_tokenizer('basic_english')\n",
    "ko_tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  8000\n",
      "valid size:  2000\n"
     ]
    }
   ],
   "source": [
    "df_shuffled=df.sample(frac=1).reset_index(drop=True) ## 모든 행을 무작위로 섞어 새로운 데이터프레임 df_shuffled을 생성.\n",
    "df = df_shuffled[:10000]\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "print('train size: ', len(train_df))\n",
    "print('valid size: ', len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ko_tokens, train_en_tokens = tokenize(train_df, ko_tokenizer, en_tokenizer)\n",
    "valid_ko_tokens, valid_en_tokens = tokenize(valid_df, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '전', '세계', '에서', '무', '비자', '입국', '이', '가능', '한', '특수', '한', '치안', '환경', ',', '불법', '체류', '자', '증가', '등', '으로', '제주', '에서', '외국인', '강력범', '죄', '가', '늘', '고', '있', '다', '.', '<eos>']\n",
      "['<sos>', 'violent', 'crimes', 'against', 'foreigners', 'are', 'on', 'the', 'rise', 'in', 'jeju', 'due', 'to', 'the', 'special', 'security', 'environment', 'that', 'allows', 'visa-free', 'entry', 'around', 'the', 'world', 'and', 'the', 'increase', 'of', 'illegal', 'residents', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(train_ko_tokens[0])\n",
    "print(train_en_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ko_vocab = build_vocab(train_ko_tokens)\n",
    "train_en_vocab = build_vocab(train_en_tokens)\n",
    "\n",
    "valid_ko_vocab = build_vocab(valid_ko_tokens)\n",
    "valid_en_vocab = build_vocab(valid_en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18587 16055\n",
      "8631 7740\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ko_vocab), len(train_en_vocab))\n",
    "print(len(valid_ko_vocab), len(valid_en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ko_indices = [tokens_to_indices(tokens, train_ko_vocab) for tokens in train_ko_tokens]\n",
    "train_en_indices = [tokens_to_indices(tokens, train_en_vocab) for tokens in train_en_tokens]\n",
    "\n",
    "valid_ko_indices = [tokens_to_indices(tokens, valid_ko_vocab) for tokens in valid_ko_tokens]\n",
    "valid_en_indices = [tokens_to_indices(tokens, valid_en_vocab) for tokens in valid_en_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 82, 246, 19, 1342, 5548, 3970, 4, 122, 17, 1607, 17, 2770, 322, 15, 1286, 4848, 96, 446, 30, 18, 1315, 19, 682, 6474, 2265, 11, 727, 14, 16, 9, 3, 2]\n",
      "[1, 3440, 1944, 255, 1411, 24, 18, 3, 1096, 9, 812, 100, 8, 3, 215, 316, 433, 12, 1923, 15577, 1823, 248, 3, 182, 7, 3, 336, 6, 1043, 200, 5, 2]\n",
      "['<sos>', '전', '세계', '에서', '무', '비자', '입국', '이', '가능', '한', '특수', '한', '치안', '환경', ',', '불법', '체류', '자', '증가', '등', '으로', '제주', '에서', '외국인', '강력범', '죄', '가', '늘', '고', '있', '다', '.', '<eos>']\n",
      "['<sos>', 'violent', 'crimes', 'against', 'foreigners', 'are', 'on', 'the', 'rise', 'in', 'jeju', 'due', 'to', 'the', 'special', 'security', 'environment', 'that', 'allows', 'visa-free', 'entry', 'around', 'the', 'world', 'and', 'the', 'increase', 'of', 'illegal', 'residents', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sample_ko = train_ko_indices[0]\n",
    "sample_en = train_en_indices[0]\n",
    "\n",
    "print(sample_ko)\n",
    "print(sample_en)\n",
    "\n",
    "sample_ko = train_ko_vocab.lookup_tokens(sample_ko)\n",
    "sample_en = train_en_vocab.lookup_tokens(sample_en)\n",
    "\n",
    "print(sample_ko)\n",
    "print(sample_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_indices, trg_indices):\n",
    "        self.src_indices = src_indices\n",
    "        self.trg_indices = trg_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sample = torch.tensor(self.src_indices[idx], dtype=torch.long)\n",
    "        trg_sample = torch.tensor(self.trg_indices[idx], dtype=torch.long)\n",
    "        return src_sample, trg_sample\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)  # 0은 패딩 인덱스\n",
    "    trg_batch_padded = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch_padded, trg_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_ko_indices, train_en_indices)\n",
    "valid_dataset = TranslationDataset(valid_ko_indices, valid_en_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    4, 1050,  ...,    0,    0,    0],\n",
      "        [   1,   44,    5,  ...,    9,    3,    2],\n",
      "        [   1,  489, 1088,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,  786,   40,  ...,    0,    0,    0],\n",
      "        [   1, 2940,    5,  ...,    0,    0,    0],\n",
      "        [   1, 2244,  373,  ...,    0,    0,    0]])\n",
      "tensor([[   1, 2908,   24,  ...,    0,    0,    0],\n",
      "        [   1,   49,   36,  ..., 8506,    5,    2],\n",
      "        [   1,  156,   28,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,  696,    4,  ...,    0,    0,    0],\n",
      "        [   1,  218,    5,  ...,    0,    0,    0],\n",
      "        [   1,    3,  917,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for ko, en in train_loader:\n",
    "    print(ko)\n",
    "    print(en)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
