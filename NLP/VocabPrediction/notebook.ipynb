{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/pervinco/DL-workspace/NLP/VocabPrediction\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from models.model import LSTM\n",
    "from utils.util import make_dir, read_file\n",
    "from data.datasets import download_wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/wikitext\"\n",
    "save_dir = \"/home/pervinco/Models/wikitext\"\n",
    "\n",
    "model_type = \"LSTM\"\n",
    "seq_len = 50\n",
    "epochs = 5000\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65\n",
    "\n",
    "tie_weights = True\n",
    "sequence_length = 50\n",
    "clip = 0.25\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "make_dir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Dataset\n",
    "download_wikitext(data_dir)\n",
    "\n",
    "## Define Dataset Dir\n",
    "data_dir = f\"{data_dir}/wikitext-2\"\n",
    "train_file = f\"{data_dir}/wiki.train.tokens\"\n",
    "valid_file = f\"{data_dir}/wiki.valid.tokens\"\n",
    "test_file = f\"{data_dir}/wiki.test.tokens\"\n",
    "\n",
    "## Read File\n",
    "train_text = read_file(train_file)\n",
    "valid_text = read_file(valid_file)\n",
    "test_text = read_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_text), len(valid_text), len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_text[:5]:\n",
    "    # print(len(data))\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(text):\n",
    "    \"\"\"\n",
    "    데이터셋에 포함된 각각의 문장들을 토큰화(문장을 단어 단위로 분리)하며, 마지막에 <eos> 토큰을 추가한다.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    for line in text:\n",
    "        tokens = tokenizer(line.strip()) + ['<eos>']\n",
    "        tokenized_data.append(tokens)\n",
    "    return tokenized_data\n",
    "\n",
    "## Tokenize\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_data_tokens = tokenize_file(train_text)\n",
    "valid_data_tokens = tokenize_file(valid_text)\n",
    "test_data_tokens = tokenize_file(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_data in train_data_tokens[:5]:\n",
    "    print(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_tokens):\n",
    "    \"\"\"\n",
    "    토큰화된 데이터셋을 이용해 단어 사전을 생성한다.\n",
    "    \"\"\"\n",
    "    return build_vocab_from_iterator(data_tokens, specials=['<unk>', '<eos>'], min_freq=3)\n",
    "\n",
    "## 단어집 생성.\n",
    "vocab = build_vocab(train_data_tokens + valid_data_tokens + test_data_tokens)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size) ## 28783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tokenized_data, vocab, batch_size):\n",
    "    \"\"\"\n",
    "    토큰들이 Vocab내 index로 mapping하고 리스트에 저장.\n",
    "    결과적으로 전체 문장들이 하나의 문장으로 모두 연결된다.\n",
    "\n",
    "    데이터의 총 원소 수(numel())를 batch_size로 나누어 전체 데이터를 몇 개의 배치로 나눌 수 있는지 계산한다. \n",
    "    이를 통해, 데이터의 길이를 배치 크기에 맞게 조정합니다. 즉, 모든 배치의 길이는 통일합니다.\n",
    "\n",
    "    ex) 전체 데이터의 수가 2086708일 때, batch_size=128 ---> 2086708 // 128 = 16302 따라서 num_batches = 16302\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for tokens in tokenized_data:\n",
    "        token_indices = [vocab[token] for token in tokens]\n",
    "        data.extend(token_indices)\n",
    "\n",
    "    data = torch.LongTensor(data) ## [2086708]\n",
    "    num_batches = data.numel() // batch_size ## 16302\n",
    "    data = data[:num_batches * batch_size] ## 2086656. 남은 52개의 단어는 제외하여 모든 데이터들의 길이를 통일시킨다.\n",
    "    data = data.view(batch_size, -1) ## [batch_size, 16302]로 reshape\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 전체 토큰의 수, 전체 batch 단위의 수\n",
    "train_data = get_data(train_data_tokens, vocab, batch_size) ## data.numel() : 2086708, num_batches : 16302, data : 218177\n",
    "valid_data = get_data(valid_data_tokens, vocab, batch_size) \n",
    "test_data = get_data(test_data_tokens, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "전체 문장을 토큰화하고, 맵핑한 길이는 2086708\n",
    "batch_size 크기로 나누면 16302의 batch가 만들어질 수 있다. [batch_size, num_batches]로 변환함으로 인해 세로축이 하나의 문장이 된다.\n",
    "\"\"\"\n",
    "print(train_data.shape) ## 128개의 단어가 담긴 텐서가 16302개 있다. We have 16302 batches, each of 128 words\n",
    "\n",
    "sample = train_data[0, :].numpy() ## 0번째 row를 기준으로 모든 column들을 가져온다. 즉, 첫번째 row를 가져온다.\n",
    "print(sample.shape)\n",
    "sample_str = \"\"\n",
    "\n",
    "for s in sample:\n",
    "    char = vocab.lookup_token(s)\n",
    "    print(char)\n",
    "    sample_str += f\"{char} \"\n",
    "\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    src = data[:, idx : idx + seq_len] ## 전체 row를 기준으로 0번째부터 50번째 column을 가져온다.                   \n",
    "    target = data[:, idx + 1 : idx + seq_len + 1] ## 전체 row를 기준으로 1번째부터 51번째 column을 가져온다.\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    \"\"\"\n",
    "    특정 시퀀스 길이(seq_len)에 대해 데이터를 추가로 조정하기 위한 과정으로, \n",
    "    모든 배치가 지정된 시퀀스 길이에 정확히 맞도록 하여, 모델이 일관된 길이의 시퀀스를 처리하도록 한다.\n",
    "    \"\"\"\n",
    "    num_batches = data.shape[-1] ## 16302\n",
    "    ## 각 배치의 시퀀스 길이가 seq_len으로 완전히 나누어 떨어지도록 조정. \n",
    "    ## 나눗셈의 나머지는 마지막 배치에서 잘라내야 할 추가적인 시퀀스 길이를 나타낸다.\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len] ## [128, 16301]\n",
    "    num_batches = data.shape[-1] ## 16301\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) ## [128, 50], [128, 50]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden) ## [128, 50, 28783]\n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   ## [6400, 28783]\n",
    "        target = target.reshape(-1) ## [6400]\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, sequence_length, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, sequence_length, device)\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/best.pt')\n",
    "\n",
    "    print(f\"Epoch : {epoch+1} | Train Prep : {math.exp(train_loss):.3f}, | Valid Pred : {math.exp(valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens] ## 토큰들을 기반으로 index 리스트를 만든다.\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len): ## 최대 max_seq_len을 초과하지 않도록 설정.\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden) ## prediction [1, seq_len, vocab_size]\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1) ## 마지막 seq_len을 선택한다.\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Think about'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# convert the code above into a for loop\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
