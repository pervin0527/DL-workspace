{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from konlpy.tag import Mecab, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 처리 과정 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Q                         A  label\n",
      "0                       12시 땡!                하루가 또 가네요.      0\n",
      "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
      "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
      "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
      "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
      "...                        ...                       ...    ...\n",
      "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
      "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
      "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
      "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
      "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
      "\n",
      "[11823 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/ChatbotData.csv'\n",
    "df = pd.read_csv(data_dir) ## Question, Answer, Label(emotion - 0 : normal, 1 : negative, 2 : positive)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823 11823\n"
     ]
    }
   ],
   "source": [
    "question = df['Q']\n",
    "answer = df['A']\n",
    "\n",
    "print(len(question), len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('[^ ?,.!A-Za-z0-9가-힣+]')\n"
     ]
    }
   ],
   "source": [
    "## 이 문자들을 제외한 나머지들은 모두 제거한다.\n",
    "korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "normalizer = re.compile(korean_pattern)\n",
    "print(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대리님이 너무 갈궈 ---> ['대리', '님', '이', '너무', '갈궈']\n"
     ]
    }
   ],
   "source": [
    "## 한글 형태소 분석\n",
    "mecab = Mecab()\n",
    "okt = Okt()\n",
    "\n",
    "rand_idx = random.randint(0, len(question) - 1)\n",
    "print(f\"{question[rand_idx]} ---> {mecab.morphs(normalize(question[rand_idx]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대리 님 이 너무 갈궈\n",
      "더 웃으면서 대해 보세요 .\n"
     ]
    }
   ],
   "source": [
    "def clean_text(sentence, tagger):\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = tagger.morphs(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "print(clean_text(question[rand_idx], okt))\n",
    "print(clean_text(answer[rand_idx], okt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', 'ppl 심하네']\n",
      "['하루 가 또 가네요 .', '위로 해 드립니다 .', '여행 은 언제나 좋죠 .', '여행 은 언제나 좋죠 .', '눈살 이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "questions = [clean_text(sent, okt) for sent in question[:1000]]\n",
    "answers = [clean_text(sent, okt) for sent in answer[:1000]]\n",
    "\n",
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 사전 생성\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2count = {}\n",
    "        \n",
    "        self.word2index = {'<PAD>' : PAD_TOKEN,\n",
    "                           '<SOS>' : SOS_TOKEN,\n",
    "                           '<EOS>' : EOS_TOKEN}\n",
    "        \n",
    "        self.index2word = {PAD_TOKEN : '<PAD>',\n",
    "                           SOS_TOKEN : '<SOS>',\n",
    "                           EOS_TOKEN : '<EOS'}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 내 능력 이 너무 모자라\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '내': 3, '능력': 4, '이': 5, '너무': 6, '모자라': 7}\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(questions) - 1)\n",
    "print(f\"Original : {questions[rand_idx]}\")\n",
    "\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(questions[rand_idx])\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 25, 81, 47, 74, 63]\n"
     ]
    }
   ],
   "source": [
    "## batch를 위해 문장의 길이를 맞춰준다. padding\n",
    "max_length = 10\n",
    "sentence_length = 6\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,))\n",
    "sentence_tokens = sentence_tokens.tolist()\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 25, 81, 47, 74, 63]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = sentence_tokens[ : (max_length - 1)]\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "print(sentence_tokens)\n",
    "print(token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : [31, 25, 81, 47, 74, 63, 2, 0, 0, 0]\n",
      "total length: 10\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens.append(2) ## <EOS> token 추가.\n",
    "for i in range(token_length, max_length - 1):\n",
    "    sentence_tokens.append(PAD_TOKEN)\n",
    "\n",
    "print(f\"output : {sentence_tokens}\")\n",
    "print(f\"total length: {len(sentence_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Train, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, min_length=3, max_length=32):\n",
    "        super(TextDataset, self).__init__()\n",
    "\n",
    "        ## Token 정의.\n",
    "        self.PAD_TOKEN = 0\n",
    "        self.SOS_TOKEN = 1\n",
    "        self.EOS_TOKEN = 2\n",
    "\n",
    "        self.tagger = Mecab() ## 형태소 분석기\n",
    "        self.max_length = max_length ## 한 문장의 최대 길이\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "\n",
    "        self.source_clean = []\n",
    "        self.target_clean = []\n",
    "        self.wordvocab = WordVocab()\n",
    "        for _, row in df.iterrows():\n",
    "            source = row['Q'] ## raw questions\n",
    "            target = row['A'] ## raw answers\n",
    "\n",
    "            source = self.clean_text(source)\n",
    "            target = self.clean_text(target)\n",
    "\n",
    "            if len(source.split()) > min_length and len(target.split()) > min_length:\n",
    "                self.wordvocab.add_sentence(source)\n",
    "                self.wordvocab.add_sentence(target)\n",
    "                self.source_clean.append(source)\n",
    "                self.target_clean.append(target)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.source_clean[idx] ## n번째 (형태소로 분리된) 문장 하나를 가져온다. ex) 내 의견 을 존중 해줬으면\n",
    "        inputs_sentences = self.texts_to_sequences(inputs) ## ex) {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '내': 3, '의견': 4, '을': 5, '존중': 6, '해줬으면': 7}\n",
    "        inputs_padded = self.pad_sequence(inputs_sentences)\n",
    "\n",
    "        outputs = self.target_clean[idx]\n",
    "        outputs_sequences = self.texts_to_sequences(outputs)\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences)\n",
    "\n",
    "        return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_clean)\n",
    "\n",
    "\n",
    "    def normalize(self, sentence):\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        sentence = self.normalize(sentence)\n",
    "        sentence = self.tagger.morphs(sentence) ## 문장을 단어 단위로 분리한다.(형태소 분석)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        return sentence\n",
    "\n",
    "\n",
    "    def texts_to_sequences(self, sentence):\n",
    "        return [self.wordvocab.word2index[w] for w in sentence.split()] ## 각 형태소를 key로, index를 부여.\n",
    "\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens):\n",
    "        sentence_tokens = sentence_tokens[ : (self.max_length - 1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "        \n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "        for i in range(token_length, (self.max_length - 1)):\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "\n",
    "        return sentence_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tensor : torch.Size([50]) \n",
      " tensor([83, 84, 51, 85, 86, 18,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "Answer Tensor : torch.Size([50]) \n",
      " tensor([87, 88, 58, 89, 63, 90, 11,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 50\n",
    "dataset = TextDataset(\"./data/ChatbotData.csv\", min_length=3, max_length=MAX_LENGTH)\n",
    "x, y = dataset[10]\n",
    "print(f\"Question Tensor : {x.shape} \\n {x}\")\n",
    "print(f\"Answer Tensor : {y.shape} \\n {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8168 2042\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50]) torch.Size([16, 50])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1.embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50]) --> torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "embedding = nn.Embedding(num_embeddings=dataset.wordvocab.n_words, embedding_dim=embedding_dim) ## embedding할 단어의 수, embedding 차원\n",
    "\n",
    "embedded = embedding(x)\n",
    "print(f\"{x.shape} --> {embedded.shape}\")\n",
    "\n",
    "# embedded = embedded.permute(1, 0, 2)\n",
    "# print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50, 32]) torch.Size([1, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 32\n",
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "output, states = lstm(embedded)\n",
    "print(output.shape, states[0].shape, states[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 16, 32]) torch.Size([1, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_dim, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, hidden = self.lstm(x)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "encoder = Encoder(dataset.wordvocab.n_words, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_layers=1)\n",
    "output, states = encoder(x)\n",
    "\n",
    "print(output.shape, states[0].shape, states[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16]) tensor([[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([1, 16, 64])\n",
      "torch.Size([1, 16, 32]) torch.Size([1, 1, 32]) torch.Size([1, 1, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16, 6417])\n"
     ]
    }
   ],
   "source": [
    "x = torch.abs(torch.randn(size=(1, 16)).long())\n",
    "print(x.shape, x)\n",
    "\n",
    "embedding_dim = 64\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "print(embedded.shape)\n",
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "output, states = lstm(embedded)\n",
    "print(output.shape, states[0].shape, states[1].shape)\n",
    "\n",
    "output_layer = nn.Linear(32, dataset.wordvocab.n_words)\n",
    "pred = output_layer(output[0])\n",
    "print(output[0].shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 16, 32]) torch.Size([1, 16, 32]) torch.Size([1, 16, 32])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "torch.Size([16, 6417]) torch.Size([1, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_dim, embedding_dim, num_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
    "        self.out = nn.Linear(hidden_dim, num_vocabs)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0) ## (1, batch_size)\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden_state)\n",
    "        output = self.out(output.squeeze(0))\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_layers=1)\n",
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "output, states = encoder(x)\n",
    "print(output.shape, states[0].shape, states[1].shape)\n",
    "\n",
    "x = torch.abs(torch.full(size=(16, ), fill_value=SOS_TOKEN).long())\n",
    "print(x)\n",
    "\n",
    "decoder_output, decoder_hidden = decoder(x, states)\n",
    "print(decoder_output.shape, decoder_hidden[0].shape, decoder_hidden[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "        decoder_input = torch.full((batch_size, ), SOS_TOKEN, device=self.device)\n",
    "\n",
    "        for t in range(0, output_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            predicted_outputs[t] = decoder_output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.argmax(1)\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "\n",
    "        return predicted_outputs.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(6417, 64)\n",
      "    (lstm): LSTM(64, 32)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(6417, 64)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (lstm): LSTM(64, 32)\n",
      "    (out): Linear(in_features=32, out_features=6417, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(num_vocabs=dataset.wordvocab.n_words,\n",
    "                  hidden_dim=32,\n",
    "                  embedding_dim=64,\n",
    "                  num_layers=1)\n",
    "\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words,\n",
    "                  hidden_dim=32,\n",
    "                  embedding_dim=64,\n",
    "                  num_layers=1)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "print(seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50]) torch.Size([16, 50])\n",
      "torch.Size([16, 50, 6417])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "output = seq2seq(x.to(device), y.to(device))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(6417, 256)\n",
      "    (lstm): LSTM(256, 512)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(6417, 256)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (lstm): LSTM(256, 512)\n",
      "    (out): Linear(in_features=512, out_features=6417, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_DIM = 512\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "encoder = Encoder(num_vocabs=NUM_VOCABS,\n",
    "                  hidden_dim=HIDDEN_DIM,\n",
    "                  embedding_dim=EMBEDDING_DIM,\n",
    "                  num_layers=1)\n",
    "\n",
    "decoder = Decoder(num_vocabs=NUM_VOCABS,\n",
    "                  hidden_dim=HIDDEN_DIM,\n",
    "                  embedding_dim=EMBEDDING_DIM,\n",
    "                  num_layers=1)\n",
    "\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.Inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "es = EarlyStopping(patience=10, \n",
    "                   delta=0.001, \n",
    "                   mode='min', \n",
    "                   verbose=True)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=2,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_func, device):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return avg_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x, y)\n",
    "            output_dim = output.size(2)\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            eval_loss += loss.item() * x.size(0)\n",
    "            \n",
    "    return eval_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_sentence(sequences, index2word):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "\n",
    "        word = index2word[p]\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)\n",
    "\n",
    "def random_evaluation(model, dataset, index2word, device, n=10):\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)      # Shuffle\n",
    "    sampled_indices = indices[:n]   # Sampling N indices\n",
    "    \n",
    "    # 샘플링한 데이터를 기반으로 DataLoader 생성\n",
    "    sampler = SubsetRandomSampler(sampled_indices)\n",
    "    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in sampled_dataloader:\n",
    "            x, y = x.to(device), y.to(device)        \n",
    "            output = model(x, y, teacher_forcing_ratio=0)\n",
    "            # output: (number of samples, sequence_length, num_vocabs)\n",
    "            \n",
    "            preds = output.detach().cpu().numpy()\n",
    "            x = x.detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(n):\n",
    "                print(f'질문   : {sequence_to_sentence(x[i], index2word)}')\n",
    "                print(f'답변   : {sequence_to_sentence(y[i], index2word)}')\n",
    "                print(f'예측답변: {sequence_to_sentence(preds[i].argmax(1), index2word)}')\n",
    "                print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 17.0216, val_loss: 0.9497\n",
      "[EarlyStopping] (Update) Best Score: 17.02159\n",
      "[EarlyStopping] (Update) Best Score: 14.36581\n",
      "[EarlyStopping] (Update) Best Score: 13.91539\n",
      "[EarlyStopping] (Update) Best Score: 13.47932\n",
      "[EarlyStopping] (Update) Best Score: 13.18599\n",
      "epoch: 6, loss: 12.8502, val_loss: 0.8486\n",
      "[EarlyStopping] (Update) Best Score: 12.85022\n",
      "[EarlyStopping] (Update) Best Score: 12.57749\n",
      "[EarlyStopping] (Update) Best Score: 12.34305\n",
      "[EarlyStopping] (Update) Best Score: 12.20058\n",
      "[EarlyStopping] (Update) Best Score: 11.93401\n",
      "epoch: 11, loss: 11.8160, val_loss: 0.8224\n",
      "[EarlyStopping] (Update) Best Score: 11.81597\n",
      "[EarlyStopping] (Update) Best Score: 11.56410\n",
      "[EarlyStopping] (Update) Best Score: 11.30364\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[EarlyStopping] (Update) Best Score: 10.96245\n",
      "[EarlyStopping] (Update) Best Score: 10.74508\n",
      "epoch: 16, loss: 10.6937, val_loss: 0.8114\n",
      "[EarlyStopping] (Update) Best Score: 10.69366\n",
      "[EarlyStopping] (Update) Best Score: 10.31548\n",
      "Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[EarlyStopping] (Update) Best Score: 10.22975\n",
      "[EarlyStopping] (Update) Best Score: 9.98796\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.98796, Current: 10.00031, Delta: 0.01235\n",
      "epoch: 21, loss: 9.6822, val_loss: 0.8203\n",
      "[EarlyStopping] (Update) Best Score: 9.68218\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[EarlyStopping] (Update) Best Score: 9.63812\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.63812, Current: 9.69776, Delta: 0.05964\n",
      "[EarlyStopping] (Update) Best Score: 9.48135\n",
      "Epoch 00024: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.48135, Current: 9.48446, Delta: 0.00311\n",
      "epoch: 26, loss: 9.3599, val_loss: 0.8260\n",
      "[EarlyStopping] (Update) Best Score: 9.35989\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.35989, Current: 9.50647, Delta: 0.14657\n",
      "Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[EarlyStopping] (Patience) 2/10, Best: 9.35989, Current: 9.38889, Delta: 0.02900\n",
      "[EarlyStopping] (Update) Best Score: 9.31672\n",
      "[EarlyStopping] (Update) Best Score: 9.30158\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n",
      "epoch: 31, loss: 9.3380, val_loss: 0.8270\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.30158, Current: 9.33802, Delta: 0.03644\n",
      "[EarlyStopping] (Update) Best Score: 9.17546\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.17546, Current: 9.19872, Delta: 0.02326\n",
      "Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[EarlyStopping] (Patience) 2/10, Best: 9.17546, Current: 9.26503, Delta: 0.08957\n",
      "[EarlyStopping] (Update) Best Score: 9.15969\n",
      "epoch: 36, loss: 9.3967, val_loss: 0.8364\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.15969, Current: 9.39673, Delta: 0.23704\n",
      "Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[EarlyStopping] (Patience) 2/10, Best: 9.15969, Current: 9.28263, Delta: 0.12294\n",
      "[EarlyStopping] (Patience) 3/10, Best: 9.15969, Current: 9.30135, Delta: 0.14166\n",
      "[EarlyStopping] (Patience) 4/10, Best: 9.15969, Current: 9.19480, Delta: 0.03511\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.9531e-06.\n",
      "[EarlyStopping] (Patience) 5/10, Best: 9.15969, Current: 9.21595, Delta: 0.05626\n",
      "epoch: 41, loss: 9.1942, val_loss: 0.8286\n",
      "[EarlyStopping] (Patience) 6/10, Best: 9.15969, Current: 9.19423, Delta: 0.03454\n",
      "[EarlyStopping] (Patience) 7/10, Best: 9.15969, Current: 9.22658, Delta: 0.06689\n",
      "Epoch 00042: reducing learning rate of group 0 to 9.7656e-07.\n",
      "[EarlyStopping] (Patience) 8/10, Best: 9.15969, Current: 9.24262, Delta: 0.08293\n",
      "[EarlyStopping] (Update) Best Score: 9.14245\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.14245, Current: 9.19001, Delta: 0.04756\n",
      "Epoch 00045: reducing learning rate of group 0 to 4.8828e-07.\n",
      "epoch: 46, loss: 9.1356, val_loss: 0.8297\n",
      "[EarlyStopping] (Update) Best Score: 9.13562\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.13562, Current: 9.20204, Delta: 0.06641\n",
      "[EarlyStopping] (Patience) 2/10, Best: 9.13562, Current: 9.38930, Delta: 0.25368\n",
      "Epoch 00048: reducing learning rate of group 0 to 2.4414e-07.\n",
      "[EarlyStopping] (Update) Best Score: 9.12503\n",
      "[EarlyStopping] (Patience) 1/10, Best: 9.12503, Current: 9.18874, Delta: 0.06371\n",
      "epoch: 51, loss: 9.2425, val_loss: 0.8283\n",
      "[EarlyStopping] (Patience) 2/10, Best: 9.12503, Current: 9.24249, Delta: 0.11746\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.2207e-07.\n",
      "[EarlyStopping] (Patience) 3/10, Best: 9.12503, Current: 9.26859, Delta: 0.14356\n",
      "[EarlyStopping] (Patience) 4/10, Best: 9.12503, Current: 9.30334, Delta: 0.17830\n",
      "[EarlyStopping] (Patience) 5/10, Best: 9.12503, Current: 9.30362, Delta: 0.17859\n",
      "Epoch 00054: reducing learning rate of group 0 to 6.1035e-08.\n",
      "[EarlyStopping] (Patience) 6/10, Best: 9.12503, Current: 9.32933, Delta: 0.20430\n",
      "epoch: 56, loss: 9.3295, val_loss: 0.8279\n",
      "[EarlyStopping] (Patience) 7/10, Best: 9.12503, Current: 9.32951, Delta: 0.20447\n",
      "[EarlyStopping] (Patience) 8/10, Best: 9.12503, Current: 9.22841, Delta: 0.10338\n",
      "Epoch 00057: reducing learning rate of group 0 to 3.0518e-08.\n",
      "[EarlyStopping] (Patience) 9/10, Best: 9.12503, Current: 9.26204, Delta: 0.13701\n",
      "[EarlyStopping] (Patience) 10/10, Best: 9.12503, Current: 9.15580, Delta: 0.03077\n",
      "[EarlyStop Triggered] Best Score: 9.12503\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "STATEDICT_PATH = '/home/pervinco/Models/seq2seq-chatbot-kor.pt'\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_dataloader, optimizer, loss_func, device)\n",
    "    \n",
    "    val_loss = evaluate(model, test_dataloader, loss_func, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), STATEDICT_PATH)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early Stop\n",
    "    es(loss)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "                   \n",
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "torch.save(model.state_dict(), f'/home/pervinco/Models/seq2seq-chatbot-kor-{best_loss:.4f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문   : 현재 를 즐기 면서 살 고 싶 다 .\n",
      "답변   : 현재 를 즐기 면서 행복 하 게 사세요 .\n",
      "예측답변: 저 는 게 잘못 인 거 같 아요 .\n",
      "==============================\n",
      "질문   : 5 년 만났 어\n",
      "답변   : 좋 은 기억 들 이 많 겠 네요 .\n",
      "예측답변: 좋 은 하 는 있 어요 .\n",
      "==============================\n",
      "질문   : 상황 이 힘들 어서 헤어진다\n",
      "답변   : 그럴 수 있 어요 .\n",
      "예측답변: 저 도 요 .\n",
      "==============================\n",
      "질문   : 그 말 하 지 말 걸\n",
      "답변   : 이불 킥 하 고 말 했 던 사실 조 차 잊어버리 세요 .\n",
      "예측답변: 좋 은 하 이 .\n",
      "==============================\n",
      "질문   : 이 별후 2 주반 이 지난 시점\n",
      "답변   : 수 많 은 것 들 이 달라졌 겠 죠 .\n",
      "예측답변: 마음 의 준비 를 찾아보 세요 .\n",
      "==============================\n",
      "질문   : 여유 가 너무 없 어\n",
      "답변   : 나 를 사랑 하 고 상대 를 사랑 하 는 여유 를 갖 게 되 길 바라 요 .\n",
      "예측답변: 저 는 노래 를 찾아보 세요 .\n",
      "==============================\n",
      "질문   : 컨디션 조절 이 안 돼\n",
      "답변   : 컨디션 조절 은 정말 중요 해요 .\n",
      "예측답변: 마음 이 이 하 는 .\n",
      "==============================\n",
      "질문   : 우연히 또 만났 어\n",
      "답변   : 우연 이 필연 이 되 기 도 합니다 .\n",
      "예측답변: 저 도 는 해 보 세요 .\n",
      "==============================\n",
      "질문   : 바래다 달 라고 말 해 볼까 ?\n",
      "답변   : 귀엽 게 부탁 해 보 세요 .\n",
      "예측답변: 마음 하 는 게 .\n",
      "==============================\n",
      "질문   : 너 는 뭐 억 었 어 ?\n",
      "답변   : 저 는 배터리 가 밥 이 예요 .\n",
      "예측답변: 잘 하 고 있 어요 .\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
