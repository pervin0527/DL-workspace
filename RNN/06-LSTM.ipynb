{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import urllib\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('sarcasm.json', <http.client.HTTPMessage at 0x7f70e3c18dc0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
    "urllib.request.urlretrieve(url, 'sarcasm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/pervinco/Datasets/sarcasm.json') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(datas)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'my', 'name', 'is', 'minjun', 'kim', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "## example\n",
    "sample = \"Hello, my name is minjun kim.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(sentences):\n",
    "    for text in sentences:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['headline'].tolist()),\n",
    "                                  specials=[\"<Unknown>\"], ## 어휘에 없는 단어들을 \"<Unknown>\"로 대체\n",
    "                                  min_freq=2,\n",
    "                                  max_tokens=1000,)\n",
    "vocab.set_default_index(vocab['<Unknown>']) ## 생성된 어휘에서 \"<UNK>\" 토큰을 기본 인덱스로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<Unknown>', \"'\", 'to', 'of', 'the', 's', 'in', ',', 'for', 'a', 'on', '.', 'and', 'with', 'is', 'trump', 'new', 'man', 'from', 'at', 'you', 't', 'it', 'about', 'this', 'by', 'after', '?', 'be', 'that', 'how', 'out', 'he', 'as', 'up', 'not', 'what', 'can', 'are', 'your', 'his', 'who', 'just', 'has', 'will', 'more', 'all', 'one', 'into', 'report', 'i', 'why', 'have', 'area', 'woman', 'over', 'donald', 'u', 'says', 'day', 'obama', 'time', 'no', 'first', 'like', 'people', 'women', 'get', 'her', 'we', 'world', 'an', 'now', 'nation', 'house', 'life', 'off', 'clinton', 'they', 'make', 'still', 'than', 'was', 'my', 'white', 'back', 'down', 'if', 'when', 'family', 'could', 'she', 'their', 'do', 'before', 'americans', 'gop', 'most', 'way', '5', 'black', 'year', 'here', 'study', 'years', 'bill', 'should', 'would', 'him', 'president', 'best', 'so', 'america', 'police', 'only', 'watch', 'school', 'show', 'american', 'really', 'being', 'but', 'know', 'home', 'mom', 'things', 'death', 'during', 'good', 'finds', 'state', ')', 'love', '(', 'or', 'say', 'video', 'going', 'hillary', 'health', 'last', '!', 'parents', 'may', 'big', 'campaign', 'against', 'every', 'too', 'don', 'need', '3', 'child', 'kids', 'party', 'gets', 'these', '10', 'getting', 'little', 'some', 'right', 'change', 'take', 'work', 'dead', 'our', 'court', 'makes', 'there', 're', 'calls', 'other', 'news', 'doesn', 'john', '000', 'through', 'while', 'own', 'want', 'never', 'look', 'dad', '2', 'takes', 'where', 'guy', 'star', 'war', 'bush', 'gay', 'see', 'local', 'next', 'stop', 'even', 'go', '--', 'its', 'real', 'week', 'college', 'election', 'won', 'again', 'dog', 'office', 'plan', 'another', 'baby', 'us', 'got', 'thing', 'around', 'game', 'made', 'them', 'help', 'children', 'couple', 'wants', 'gun', 'debate', 'job', 'live', 'million', '4', 'actually', 'care', 'night', 'congress', 'north', 'finally', 'high', 'much', '6', 'been', 'ever', 'me', 'god', 'men', 'sex', 'son', 'under', 'money', 'national', 'teen', '7', 'season', 'bad', 'climate', 'friend', 'two', 'ways', 'better', 'give', 'media', 'top', 'city', 'everyone', 'senate', 'shows', 'students', 'had', 'let', 'paul', 'shooting', 'm', 'reveals', 'sexual', 'story', 'trying', 'facebook', 'history', 'making', 'york', 'announces', 'away', 'food', 'old', 'supreme', 'without', 'business', 'pope', 'attack', 'fight', 'mother', 'any', 'deal', 'enough', 'friends', 'girl', 'movie', 'tv', 'face', 'end', 'film', 'introduces', 'company', 'government', 'great', 'law', 'sanders', 'call', 'does', 'fire', 'part', 'tell', 'entire', 'former', '&', 'body', 'book', 'find', 'found', 'wedding', 'come', 'father', 'think', 'pretty', 'll', 'republican', '8', 'speech', 'support', 'coming', 'single', 'future', 'morning', 'photos', 'presidential', 'public', 'self', 'car', 'd', 'james', 'keep', 'republicans', 'run', 'use', 'power', 'thinks', 'very', 'already', 'christmas', 'didn', 'name', 'email', 'rights', 'talk', '2016', 'case', 'democrats', 'marriage', 'student', 'violence', 'behind', 'between', 'country', 'line', 'race', 'releases', 'boy', 'killed', 'ryan', 'tax', 'teacher', 'voters', 'doing', 'group', 'human', 'might', 'secret', 'security', 'something', 'vote', 'california', 'fans', 'goes', 'having', 'long', 'used', '1', 'must', 'today', 'asks', 'bernie', 'free', 'team', 'twitter', 'ban', 'department', 'wife', 'win', 'looking', 'open', 'poll', 'ad', 'because', 'girls', 'middle', 'room', 'biden', 'each', 'judge', 'obamacare', 'once', 'sure', 've', 'days', 'inside', 'minutes', 'political', 'art', 'claims', 'control', 'everything', 'forced', 'music', 'super', 'daughter', 'many', 'meet', 'missing', 'perfect', 'running', 'save', 'states', 'person', 'second', 'times', 'unveils', '20', 'candidate', 'ceo', 'living', 'plans', 'put', 'reports', 'same', 'scientists', 'social', 'summer', 'always', 'employee', 'justice', 'photo', 'tells', 'dies', 'red', 'until', 'did', 'female', 'full', 'korea', 'looks', 'talks', 'texas', 'were', 'comes', 'cruz', 'head', 'michael', 'needs', 'pay', 'ready', 'romney', 'russia', 'secretary', 'warns', '12', 'admits', 'cancer', 'kim', 'list', 'lives', 'mike', 'past', 'taking', 'town', 'wall', 'water', 'crisis', 'earth', 'gives', 'idea', 'record', 'set', 'working', 'wrong', 'administration', 'director', 'george', 'hot', 'letter', 'restaurant', 'shot', 'thought', 'heart', 'iran', 'left', 'someone', 'wins', 'dream', 'hours', 'meeting', 'probably', 'service', 'south', 'start', 'young', 'age', 'education', 'fan', 'fbi', 'hollywood', 'lost', 'nothing', 'percent', 'rock', 'street', 'tips', 'cat', 'china', 'isis', 'kill', 'king', 'owner', 'place', 'thousands', 'three', 'washington', 'believe', 'breaking', 'few', 'fucking', 'phone', 'together', 'tweets', 'chief', 'eating', 'feel', 'florida', 'francis', 'internet', 'march', 'officials', 'stars', 'talking', 'yet', 'attacks', 'chris', 'class', 'federal', 'guide', 'leaves', 'military', 'online', 'order', 'questions', 'ted', 'birthday', 'drug', 'move', 'air', 'ask', 'beautiful', 'congressman', 'giving', 'kid', 'latest', 'months', 'problem', 'reason', 'sleep', 'small', '9', 'assault', 'buy', 'different', 'happy', 'huffpost', 'less', 'majority', 'nuclear', 'personal', 'rules', 'series', 'word', '-', '2015', 'democratic', 'fox', 'girlfriend', 'holiday', 'isn', 'lot', 'moment', 'month', 'outside', 'prison', 'reasons', 'shit', 'system', 'those', 'told', 'community', 'favorite', 'hard', 'kind', 'leave', 'response', 'rise', 'travel', 'bar', 'fun', 'interview', 'issues', 'jimmy', 'mark', 'play', 'relationship', 'senator', 'box', 'celebrates', 'excited', 'huge', 'ice', 'immigration', 'leaders', 'read', 'scott', 'special', 'weekend', 'cop', 'cover', 'following', 'gift', 'hope', 'knows', 'message', 'o', 'offers', 'pence', 'politics', 'protest', 'stephen', 'syria', 'thinking', 'tom', 'trailer', 'trip', 'union', 'using', 'visit', 'watching', 'david', 'hate', 'hit', 'leader', 'lessons', 'muslim', 'russian', 'since', 'straight', 'taylor', 'victims', 'accused', 'anything', 'apple', 'bring', 'conversation', 'front', 'hair', 'himself', 'adorable', 'birth', 'break', 'candidates', 'chinese', 'cops', 'date', 'drunk', 'joe', 'least', 'millions', 'prince', 'queer', 'united', 'words', 'become', 'billion', 'fall', 'industry', 'investigation', 'learned', 'mass', 'opens', 'powerful', 'reality', 'rubio', 'third', 'totally', 'whole', 'career', 'die', 'early', 'employees', 'friday', 'iraq', 'returns', 'true', 'waiting', '11', '15', 'almost', 'amazon', 'artist', 'breaks', 'fashion', 'feels', 'kardashian', 'massive', 'point', 'policy', 'schools', 'stage', 'syrian', 'turn', 'wars', 'well', 'abortion', 'awards', 'cnn', 'completely', 'coworker', 'dance', 'dating', 'dinner', 'hits', 'jenner', 'jr', 'killing', 'kills', 'murder', 'puts', 'song', 'spends', 'sports', 'turns', 'wearing', 'west', 'workers', 'worried', '50', 'center', 'crash', 'experience', 'final', 'global', 'host', 'late', 'names', 'post', 'sick', 'sign', 'signs', 'vows', 'weird', 'worst', '30', 'adds', 'anniversary', 'called', 'chicago', 'decision', 'ferguson', 'fighting', 'governor', 'hands', 'j', 'key', 'moving', 'netflix', 'nfl', 'reportedly', 'seen', 'store', 'surprise', 'trans', 'transgender', 'university', '2014', '2017', 'across', 'act', 'advice', 'band', 'bus', 'c', 'discover', 'force', 'google', 'keeps', 'magazine', 'michelle', 'moms', 'official', 'paris', 'return', 'road', 'struggling', 'suicide', 'voice', '9/11', 'brings', 'executive', 'fear', 'harry', 'important', 'kerry', 'lead', 'light', 'longer', 'lose', 'nyc', 'peace', 'planned', 'reform', 'suspect', 'users', 'walking', 'apartment', 'audience', 'biggest', 'carolina', 'coffee', 'eat', 'football', 'given', 'hall', 'hoping', 'iowa', 'jobs', 'members', 'oil', 'oscars', 'program', 'question', 'stand', 'starting', 'steve', 'swift', 'test', 'wait', 'weight', 'whether', 'worth', 'beauty', 'chance', 'doctor', 'general', 'grandma', 'major', 'mind', 'planet', 'possible', 'protesters', 'remember', 'risk', 'success', 'which', 'abuse', 'allegations', 'clearly', 'colbert', 'cool', 'cut', 'demands', 'five', 'fuck', 'halloween', 'hand', 'hear', 'hurricane', 'ideas', 'israel', 'mental', 'moore', 'park', 'playing', 'poor', 'press', 'queen', 'role', 'space', 'teens', 'tour', 'try', 'williams', 'asking', 'ben', 'board', 'celebrate', 'dying', 'far', 'green', 'hero', 'hilarious', 'homeless', 'officer', 'oscar', 'plane', 'pregnant', 'problems', 'push', 'reporter', 'sean', 'side', 'supporters', 'table', 'throws', 'vacation', 'voter', 'website', 'wishes', 'album', 'amazing', 'begins', 'boss', 'building', 'card', 'chicken', 'church', 'data', 'deadly', 'economy', 'emotional', 'epa', 'eyes', 'fails', 'families', 'feeling', 'happened', 'humans', 'kelly', 'leads', 'learn', 'manager', 'marijuana', 'number', 'picture', 'pizza', 'proud', 'receives', 'refugees', 'rest', 'results', 'reveal', 'robert', 'shares', 'simple', 'snl', 'suggests', 'urges', 'uses', '13', 'address', 'amid', 'apparently', 'arrested', 'boyfriend', 'boys', 'check', 'close', 'comey', 'demand', 'desperate', 'door', 'driver', 'driving', 'easy', 'finding', 'hour', 'leaving', 'lgbt', 'male', 'older', 'performance', 'quietly', 'racist', 'rally', 'sales', 'shop', 'went', 'worse', 'actor', 'al', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "str_to_idx = vocab.get_stoi()\n",
    "idx_to_str = vocab.get_itos()\n",
    "\n",
    "print(idx_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 83, 347, 14, 0, 474, 11]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(tokenizer(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "84\n",
      "the 'roseanne' revival catches up to our thorny political mood, for better and worse\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[\"headline\"],\n",
    "                                                    df['is_sarcastic'],\n",
    "                                                    stratify=df['is_sarcastic'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "print(len(x_train[0]))\n",
    "print(x_train[0])\n",
    "\n",
    "print(len(x_train[1]))\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        return self.vocab(self.tokenizer(text)), label\n",
    "    \n",
    "def collate_batch(batch, max_sequence_length):\n",
    "    label_list, text_list = [], []\n",
    "\n",
    "    for text, label in batch:\n",
    "        processed_text = torch.tensor(text[:max_sequence_length], dtype=torch.int64) ## 길이를 max_sequence_length를 넘지 못하게 만든다.\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0) ## padding을 통해 데이터의 길이를 일정하게 맞춰준다.\n",
    "\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train, vocab=vocab, tokenizer=tokenizer)\n",
    "valid_dataset = CustomDataset(x_test, y_test, vocab=vocab, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0, 0, 262, 142, 214, 925, 186, 32, 0], 1)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x : collate_batch(x, MAX_SEQUENCE_LEN)) ## 최대 길이가 100\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x : collate_batch(x, MAX_SEQUENCE_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 22]) torch.Size([32])\n",
      "tensor([[  0, 467,   1,   5, 863, 522,  38,   1,   0,   1,   0,   1,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [191,   0,   0,   3, 120,   0,   2,  33,   1,  29, 191,   0,   1,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 526,   0,   0,   0,   0,  19,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [147,  47,   3,  17,   1,   5,   0,   0,   2,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 83,   0, 622,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0, 786,   0, 652,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [946,   0,   1,   5, 608, 522,   8,   0, 807,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,  19,   4,   0, 483,  32,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [275, 408, 545,   0, 174,   1,  21,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0, 417,   0,   0, 107,   0,   0,   7, 176,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 14,  83, 633, 152, 120,   0,  27,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [589,   0,  33,   0,  36, 168,   4, 173, 982,   2,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,  21,  11,   9,  11,  35,   0,   6,  57,  11,   5,  11,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,  10,   0,   0,   0,  80,   0,   3,  47,  59, 158,   0, 494,\n",
      "         206,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  4,   0,   3,   0,   9, 126,   3,   9,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 457,   0,  12,  40,  63,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [534,   0, 609,  33,   9, 536, 520, 161,  72,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0, 367,   0,   0,  31,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 992,   2, 670,   1,   5,   0,  55, 515, 935, 977,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [493,  44,   0, 105, 642,   1,   0,   1,   0,   7,  32,  14,  35,   1,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0, 449, 127,   0,   4,  70,   0, 361,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [458,   0,  11,   0,  11,  35, 754,  23,   0, 105,   0,   4, 130, 245,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [467, 922,   0,   8,   0,   6,   0,   0,   2,  67,   0, 376,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,  10,   0, 207,   0,  26,  15,   0,   0,   0,  10,   0,\n",
      "           3,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  8, 852,   0,   7, 245, 596,   1,  21, 417,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [109,  15,   1,   5, 500, 289, 641,   2, 232, 455,  93,  35,   0, 532,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 330,   3, 243,   0,  10, 206,   0, 558,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,  18,   0,   0,   1,   5, 268,  11,   5,  11,   1,   0,   1,   0,\n",
      "           0,   0,   0,  29,   0,  48,   0,   0],\n",
      "        [173,   0,   8,   0,   0,   7, 794,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [144, 902, 488, 715, 412,   0,   2, 256,   0,   9, 159,   0,  61,   6,\n",
      "           4, 329,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   6,  33, 112,   1,   5,   0, 281, 167, 446,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [206,  13,   0,   0,  43,   9,   0, 478,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    tokens, labels = data[0], data[1]\n",
    "    print(tokens.shape, labels.shape)\n",
    "    print(tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCAB = len(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1000, 30)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "embedding = nn.Embedding(len(vocab), EMBEDDING_DIM).to(device) ## len(vocab)개의 단어들을 EMBEDDING_DIM 크기의 실수 벡터로 변환하는 임베딩을 생성.\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21, 30])\n",
      "tensor([[[ 1.5194,  0.5919,  1.2933,  ...,  0.7421, -1.0791, -1.8894],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [-1.9865, -0.7665,  1.3789,  ...,  0.1180,  0.6861, -1.3133],\n",
      "         ...,\n",
      "         [-0.4216, -0.4395, -0.5644,  ...,  0.5132,  0.0333, -1.7277],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]],\n",
      "\n",
      "        [[ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         ...,\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]],\n",
      "\n",
      "        [[ 0.9251,  0.2349, -0.9838,  ...,  0.9991, -0.0461,  0.6726],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         ...,\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4686, -0.9682,  1.8036,  ...,  0.0435,  0.4819, -0.7796],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [-0.9904,  0.7791,  0.3047,  ...,  0.2203, -2.1367,  1.0662],\n",
      "         ...,\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]],\n",
      "\n",
      "        [[ 0.8852,  1.3288, -0.0350,  ..., -0.2772,  0.3837,  0.8211],\n",
      "         [-0.0884,  0.4436, -0.0625,  ...,  0.0268,  0.2203,  1.4971],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         ...,\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]],\n",
      "\n",
      "        [[ 1.1948,  0.7010, -0.7388,  ...,  0.3549, -0.8323, -1.2729],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         ...,\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443],\n",
      "         [ 0.8243,  0.7813, -0.7459,  ..., -0.2022, -0.6529, -1.1443]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_out = embedding(x)\n",
    "print(embedding_out.shape)\n",
    "print(embedding_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(30, 64, batch_first=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = 1\n",
    "SEQ_LENGTH = x.size(1)\n",
    "\n",
    "lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, batch_first=True, device=device)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)\n",
    "c_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21, 64])\n",
      "torch.Size([1, 32, 64]) torch.Size([1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "lstm_out, (hidden, cell) = lstm(embedding_out)\n",
    "print(lstm_out.shape)\n",
    "print(hidden.shape, cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingLSTM(x, vocab_size, embedding_dim, hidden_size, bidrectional, num_layers, device):\n",
    "    print(f\"Input Shape : {x.shape}\")\n",
    "    x = x.to(device)\n",
    "    \n",
    "    \n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
    "    embedding_out = embedding(x)\n",
    "    print(f\"Embedded Input Shape : {embedding_out.shape}\")\n",
    "\n",
    "    lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                   hidden_size=hidden_size,\n",
    "                   num_layers=num_layers,\n",
    "                   batch_first=True,\n",
    "                   device=device)\n",
    "    \n",
    "    bidi = 2 if bidrectional else 1\n",
    "    out, (h, c) = lstm(embedding_out)\n",
    "    print(f\"Output Shape : {out.shape}\")\n",
    "    print(f\"Hidden Shape : {h.shape}\")\n",
    "    print(f\"Cell State Shape : {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape : torch.Size([32, 21])\n",
      "Embedded Input Shape : torch.Size([32, 21, 30])\n",
      "Output Shape : torch.Size([32, 21, 64])\n",
      "Hidden Shape : torch.Size([2, 32, 64])\n",
      "Cell State Shape : torch.Size([2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "EmbeddingLSTM(x, vocab_size=len(vocab), embedding_dim=30, hidden_size=64, bidrectional=False, num_layers=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, hidden_size, num_layers, bidirectional=True, drop_prob=0.1):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                           )\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def init_hidden_and_cell_state(self, batch_size, device):\n",
    "        # LSTM 입력시 초기 Cell 에 대한 가중치 초기화를 진행합니다.\n",
    "        # (num_layers*bidirectional, batch_size, hidden_size)\n",
    "        self.hidden_and_cell = (\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x, self.hidden_and_cell)\n",
    "        # (batch_size, seq_length, hidden_size*bidirectional)\n",
    "        # last sequence 의 (batch_size, hidden_size*bidirectional)\n",
    "        h = output[:, -1, :]\n",
    "        o = self.dropout(h)\n",
    "        o = self.relu(self.fc(o))\n",
    "        o = self.dropout(o)\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): Embedding(1000, 16)\n",
       "  (lstm): LSTM(16, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'num_classes': 2, \n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 16, \n",
    "    'hidden_size': 32, \n",
    "    'num_layers': 2, \n",
    "    'bidirectional': True,\n",
    "}\n",
    "\n",
    "model = TextClassificationModel(**config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 정의: CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimizer, device):\n",
    "    # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "    model.train()\n",
    "    \n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    \n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for idx, (txt, lbl) in enumerate(prograss_bar):\n",
    "        # txt, lbl 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "        txt = txt.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        \n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # LSTM Weight 초기화\n",
    "        model.init_hidden_and_cell_state(len(txt), device)\n",
    "        \n",
    "        # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "        output = model(txt)\n",
    "        \n",
    "        # 손실함수에 output, lbl 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, lbl)\n",
    "        \n",
    "        # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Probability Max index 를 구합니다.\n",
    "        output = output.argmax(dim=1)\n",
    "        \n",
    "        # 정답 개수를 구합니다.\n",
    "        corr += (output == lbl).sum().item()\n",
    "        counts += len(lbl)\n",
    "        \n",
    "        # batch 별 loss 계산하여 누적합을 구합니다.\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 프로그레스바에 학습 상황 업데이트\n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "        \n",
    "    # 누적된 정답수를 전체 개수로 나누어 주면 정확도가 산출됩니다.\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    \n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다. \n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "    \n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for txt, lbl in data_loader:\n",
    "            # txt, lbl 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "            txt = txt.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "            \n",
    "            # LSTM Weight 초기화\n",
    "            model.init_hidden_and_cell_state(len(txt), device)\n",
    "    \n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(txt)\n",
    "            \n",
    "            # 검증 손실을 구합니다.\n",
    "            loss = loss_fn(output, lbl)\n",
    "            \n",
    "            # Probability Max index 를 구합니다.\n",
    "            output = output.argmax(dim=1)\n",
    "            \n",
    "            # 정답 개수를 구합니다.\n",
    "            corr += (output == lbl).sum().item()\n",
    "            \n",
    "            # batch 별 loss 계산하여 누적합을 구합니다.\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # validation 정확도를 계산합니다.\n",
    "        # 누적한 정답숫자를 전체 데이터셋의 숫자로 나누어 최종 accuracy를 산출합니다.\n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        \n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.67011, training accuracy: 0.58249: 100%|██████████| 668/668 [00:01<00:00, 438.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.59229. Saving Model!\n",
      "epoch 01, loss: 0.67011, acc: 0.58249, val_loss: 0.59229, val_accuracy: 0.69843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.51376, training accuracy: 0.75149: 100%|██████████| 668/668 [00:01<00:00, 478.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.59229 to 0.47432. Saving Model!\n",
      "epoch 02, loss: 0.51376, acc: 0.75149, val_loss: 0.47432, val_accuracy: 0.76675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.43716, training accuracy: 0.79543: 100%|██████████| 668/668 [00:01<00:00, 468.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.47432 to 0.43340. Saving Model!\n",
      "epoch 03, loss: 0.43716, acc: 0.79543, val_loss: 0.43340, val_accuracy: 0.79633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.39646, training accuracy: 0.81818: 100%|██████████| 668/668 [00:01<00:00, 470.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.43340 to 0.41255. Saving Model!\n",
      "epoch 04, loss: 0.39646, acc: 0.81818, val_loss: 0.41255, val_accuracy: 0.80663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.37130, training accuracy: 0.83343: 100%|██████████| 668/668 [00:01<00:00, 488.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.41255 to 0.40428. Saving Model!\n",
      "epoch 05, loss: 0.37130, acc: 0.83343, val_loss: 0.40428, val_accuracy: 0.81056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.35089, training accuracy: 0.83975: 100%|██████████| 668/668 [00:01<00:00, 493.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.40428 to 0.39339. Saving Model!\n",
      "epoch 06, loss: 0.35089, acc: 0.83975, val_loss: 0.39339, val_accuracy: 0.81580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.33484, training accuracy: 0.84996: 100%|██████████| 668/668 [00:01<00:00, 510.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.39339 to 0.38385. Saving Model!\n",
      "epoch 07, loss: 0.33484, acc: 0.84996, val_loss: 0.38385, val_accuracy: 0.82347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.32160, training accuracy: 0.85515: 100%|██████████| 668/668 [00:01<00:00, 507.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08, loss: 0.32160, acc: 0.85515, val_loss: 0.41028, val_accuracy: 0.81805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.30820, training accuracy: 0.86128: 100%|██████████| 668/668 [00:01<00:00, 500.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09, loss: 0.30820, acc: 0.86128, val_loss: 0.40018, val_accuracy: 0.83077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.29848, training accuracy: 0.86788: 100%|██████████| 668/668 [00:01<00:00, 470.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.38385 to 0.38335. Saving Model!\n",
      "epoch 10, loss: 0.29848, acc: 0.86788, val_loss: 0.38335, val_accuracy: 0.82890\n"
     ]
    }
   ],
   "source": [
    "# 최대 Epoch을 지정합니다.\n",
    "num_epochs = 10\n",
    "\n",
    "# checkpoint로 저장할 모델의 이름을 정의 합니다.\n",
    "model_name = 'LSTM-Text-Classification'\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "# Epoch 별 훈련 및 검증을 수행합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    # Model Training\n",
    "    # 훈련 손실과 정확도를 반환 받습니다.\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "    # 검증 손실과 검증 정확도를 반환 받습니다.\n",
    "    val_loss, val_acc = evaluate(model, valid_dataloader, loss_fn, device)   \n",
    "    \n",
    "    # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'/home/pervinco/Models/LSTM/{model_name}.pth')\n",
    "    \n",
    "    # Epoch 별 결과를 출력합니다.\n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
