{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchtext import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from utils import data_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(data, fname):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_pkl(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30k:\n",
    "    UNK, UNK_IDX = \"<unk>\", 0\n",
    "    PAD, PAD_IDX = \"<pad>\", 1\n",
    "    SOS, SOS_IDX = \"<sos>\", 2\n",
    "    EOS, EOS_IDX = \"<eos>\", 3\n",
    "    SPECIALS = {UNK : UNK_IDX, PAD : PAD_IDX, SOS : SOS_IDX, EOS : EOS_IDX}\n",
    "\n",
    "    def __init__(self, data_dir, target_language, max_seq_len, min_freq):\n",
    "        self.data_dir = f\"{data_dir}/Multi30k\"\n",
    "        self.cache_dir = f\"{self.data_dir}/caches\"\n",
    "\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            data_download(self.data_dir)\n",
    "\n",
    "        self.target_language = target_language\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        self.target_tokenizer = self.build_tokenizer(self.target_language)\n",
    "\n",
    "        self.train, self.valid, self.test = None, None, None\n",
    "        self.build_dataset()\n",
    "\n",
    "        self.vocab = None\n",
    "        self.build_vocab()\n",
    "\n",
    "        self.vocab_transform = None\n",
    "        self.build_transform()\n",
    "\n",
    "\n",
    "    def build_tokenizer(self, language):\n",
    "        spacy_lang_dict = {'en': \"en_core_web_sm\", 'de': \"de_core_news_sm\"}\n",
    "        assert language in spacy_lang_dict.keys()\n",
    "\n",
    "        return get_tokenizer(\"spacy\", spacy_lang_dict[language])\n",
    "\n",
    "\n",
    "    def build_dataset(self):\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "\n",
    "        train_pkl = f\"{self.cache_dir}/train.pkl\"\n",
    "        if os.path.exists(train_pkl):\n",
    "            self.train = load_pkl(train_pkl)\n",
    "\n",
    "        else:\n",
    "            with open(f\"{self.data_dir}/train.en\") as f:\n",
    "                self.train = [text.rstrip() for text in f]\n",
    "        \n",
    "            save_pkl(self.train, train_pkl)\n",
    "\n",
    "        val_pkl = f\"{self.cache_dir}/val.pkl\"\n",
    "        if os.path.exists(val_pkl):\n",
    "            self.val = load_pkl(val_pkl)\n",
    "            \n",
    "        else:\n",
    "            with open(f\"{self.data_dir}/val.en\") as f:\n",
    "                self.val = [text.rstrip() for text in f]\n",
    "        \n",
    "            save_pkl(self.val, val_pkl)\n",
    "\n",
    "        test_pkl = f\"{self.cache_dir}/test.pkl\"\n",
    "        if os.path.exists(test_pkl):\n",
    "            self.test = load_pkl(test_pkl)\n",
    "            \n",
    "        else:\n",
    "            with open(f\"{self.data_dir}/test.en\") as f:\n",
    "                self.test = [text.rstrip() for text in f]\n",
    "        \n",
    "            save_pkl(self.test, test_pkl)\n",
    "\n",
    "\n",
    "    def build_vocab(self):\n",
    "        assert self.train is not None\n",
    "\n",
    "        def yield_tokens():\n",
    "            for text in self.train:\n",
    "                yield [str(token) for token in self.target_tokenizer(text)]\n",
    "\n",
    "        vocab_file = f\"{self.cache_dir}/vocab_{self.target_language}.pkl\"\n",
    "        if os.path.exists(vocab_file):\n",
    "            vocab = load_pkl(vocab_file)\n",
    "        else:\n",
    "            vocab = build_vocab_from_iterator(yield_tokens(), min_freq=self.min_freq, specials=self.SPECIALS.keys())\n",
    "            vocab.set_default_index(self.UNK_IDX)\n",
    "            save_pkl(vocab, vocab_file)\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "    def build_transform(self):\n",
    "        def get_transform(self, vocab):\n",
    "            return transforms.Sequential(transforms.VocabTransform(vocab),\n",
    "                                         transforms.Truncate(self.max_seq_len-2),\n",
    "                                         transforms.AddToken(token=self.SOS_IDX, begin=True),\n",
    "                                         transforms.AddToken(token=self.EOS_IDX, begin=False),\n",
    "                                         transforms.ToTensor(padding_value=self.PAD_IDX))\n",
    "\n",
    "        self.vocab_transform = get_transform(self, self.vocab)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        trg = [self.target_tokenizer(data) for data in batch]\n",
    "        batch_trg = self.vocab_transform(trg)\n",
    "\n",
    "        return batch_trg, batch\n",
    "\n",
    "\n",
    "    def get_iter(self, **kwargs):\n",
    "        if self.vocab_transform is None:\n",
    "            self.build_transform()\n",
    "\n",
    "        train_iter = DataLoader(self.train, collate_fn=self.collate_fn, **kwargs)\n",
    "        valid_iter = DataLoader(self.valid, collate_fn=self.collate_fn, **kwargs)\n",
    "        test_iter = DataLoader(self.test, collate_fn=self.collate_fn, **kwargs)\n",
    "\n",
    "        return train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6191 6191\n",
      "tensor([[   2,   19,   25,   15, 1169,  808,   17,   57,   84,  336, 1339,    5,\n",
      "            3,    1,    1,    1,    1],\n",
      "        [   2,  165,   36,    7,  335,  287,   17, 1224,    4,  758, 4496, 2957,\n",
      "            5,    3,    1,    1,    1],\n",
      "        [   2,    6,   61,   33,  232,   71,    4,  253, 4460,    5,    3,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   2,    6,   12,    7,    4,   30,   23,   10,   37,    9,    4,  589,\n",
      "          586,    4,  242,    5,    3]])\n",
      "['Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.', 'A man in a blue shirt is standing on a ladder cleaning a window.']\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min([os.cpu_count(), BATCH_SIZE if BATCH_SIZE > 1 else 0, 8])\n",
    "\n",
    "DATASET = Multi30k(\"/home/pervinco/Datasets/test\", \"en\", 256, 2)\n",
    "train_iter, valid_iter, test_iter = DATASET.get_iter(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "INPUT_DIM = len(DATASET.vocab)\n",
    "OUTPUT_DIM = len(DATASET.vocab)\n",
    "print(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "for batch_data, sentence_data in train_iter:\n",
    "    print(batch_data)\n",
    "    print(sentence_data)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLanguageModel(\n",
      "  (embedding): Embedding(6191, 256, padding_idx=1)\n",
      "  (rnn): RNN(256, 512)\n",
      "  (fc): Linear(in_features=512, out_features=6191, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src shape: [src_len, batch_size]\n",
    "        embedded = self.embedding(src)\n",
    "        # embedded shape: [src_len, batch_size, emb_dim]\n",
    "        output, _ = self.rnn(embedded)\n",
    "        # output shape: [src_len, batch_size, hidden_dim]\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "model = RNNLanguageModel(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, DATASET.PAD_IDX)\n",
    "model = model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=DATASET.PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_iter)\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 24\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_iter, optimizer, criterion)\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output_dim)\n\u001b[0;32m---> 12\u001b[0m Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, Y)\n\u001b[1;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for idx, (X, Y) in enumerate(iterator):\n",
    "        X = X.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        Y = Y.view(-1)\n",
    "\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_iter)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
