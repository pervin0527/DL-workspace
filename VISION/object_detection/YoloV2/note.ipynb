{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_transform(is_train, img_size):\n",
    "    if is_train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5)\n",
    "            ], p=1),\n",
    "\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.Blur(p=0.5), \n",
    "                A.GaussianBlur(p=0.5), \n",
    "                A.GlassBlur(sigma=0.7, max_delta=4, iterations=2, p=0.5)\n",
    "            ], p=0.5),\n",
    "            \n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "                A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.5)\n",
    "            ], p=0.5),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, brightness_coeff=2.5, p=0.5),\n",
    "                A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, p=0.5)\n",
    "            ], p=0.5),\n",
    "\n",
    "            # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            # ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=[]))\n",
    "\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            # ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=[]))\n",
    "\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16551\n",
      "torch.Size([3, 416, 416])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_sets, years, use_diff=False, img_size=416):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_sets = image_sets\n",
    "        self.years = years\n",
    "        self.use_diff = use_diff\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.classes = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', \n",
    "                        'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "        self.class_to_ind = dict(zip(self.classes, range(len(self.classes))))\n",
    "        \n",
    "        self.images, self.annots = [], []\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_image_set_index(self, image_set, year):\n",
    "        image_set_file = f\"{self.root_dir}/VOC{year}/ImageSets/Main/{image_set}.txt\"\n",
    "        with open(image_set_file) as f:\n",
    "            image_index = [f\"{self.root_dir}/VOC{year}/JPEGImages/{x.strip()}.jpg\" for x in f.readlines()]\n",
    "\n",
    "        return image_index\n",
    "    \n",
    "\n",
    "    def load_pascal_annotation(self, index):\n",
    "        filename = index.replace(\"JPEGImages\", \"Annotations\").replace(\".jpg\", \".xml\")\n",
    "        tree = ET.parse(filename)\n",
    "        objs = tree.findall('object')\n",
    "\n",
    "        if not self.use_diff:\n",
    "            non_diff_objs = [obj for obj in objs if int(obj.find('difficult').text) == 0]\n",
    "            objs = non_diff_objs\n",
    "\n",
    "        num_objs = len(objs)\n",
    "        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n",
    "        gt_classes = np.zeros((num_objs), dtype=np.int32)\n",
    "\n",
    "        for ix, obj in enumerate(objs):\n",
    "            bbox = obj.find('bndbox')\n",
    "            x1 = float(bbox.find('xmin').text) - 1\n",
    "            y1 = float(bbox.find('ymin').text) - 1\n",
    "            x2 = float(bbox.find('xmax').text) - 1\n",
    "            y2 = float(bbox.find('ymax').text) - 1\n",
    "\n",
    "            cls = self.class_to_ind[obj.find('name').text.lower().strip()]\n",
    "            boxes[ix, :] = [x1, y1, x2, y2]\n",
    "            gt_classes[ix] = cls\n",
    "\n",
    "        return {'boxes': boxes, 'gt_classes': gt_classes}\n",
    "\n",
    "\n",
    "    def load_annotation_set_index(self, image_set, year, image_index):\n",
    "        cache_file = f\"{self.root_dir}/VOC{year}/{image_set}.pkl\"\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                roidb = pickle.load(f)\n",
    "\n",
    "            return roidb\n",
    "        \n",
    "        gt_roidb = [self.load_pascal_annotation(index) for index in image_index]\n",
    "        with open(cache_file, 'wb') as fid:\n",
    "            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        return gt_roidb\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        for year in self.years:\n",
    "            for image_set in self.image_sets:\n",
    "                image_index = self.load_image_set_index(image_set, year)\n",
    "                annot_index = self.load_annotation_set_index(image_set, year, image_index)\n",
    "                self.images.extend(image_index)\n",
    "                self.annots.extend(annot_index)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        annot = self.annots[idx]\n",
    "        boxes, gt_classes = annot[\"boxes\"], annot[\"gt_classes\"]\n",
    "\n",
    "        # image = Image.open(image)\n",
    "        # image = image.convert('RGB')\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        im_info = torch.FloatTensor([image.shape[0], image.shape[1]])\n",
    "\n",
    "        if \"train\" in  self.image_sets:\n",
    "            transform = get_transform(is_train=True, img_size=self.img_size)\n",
    "            transformed = transform(image=np.array(image), bboxes=boxes)\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = np.array(transformed[\"bboxes\"])\n",
    "\n",
    "            w, h = image.shape[:2]\n",
    "            boxes[:, 0::2] = np.clip(boxes[:, 0::2] / w, 0.001, 0.999)\n",
    "            boxes[:, 1::2] = np.clip(boxes[:, 1::2] / h, 0.001, 0.999)\n",
    "\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255\n",
    "            boxes = torch.from_numpy(boxes)\n",
    "            gt_classes = torch.from_numpy(gt_classes)\n",
    "            num_obj = torch.Tensor([boxes.size(0)]).long()\n",
    "            \n",
    "            return image, boxes, gt_classes, num_obj\n",
    "\n",
    "        else:\n",
    "            transform = get_transform(is_train=False, img_size=self.img_size)\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255\n",
    "            \n",
    "            return image, im_info\n",
    "\n",
    "\n",
    "# 사용 예시:\n",
    "train_dataset = VOCDataset(root_dir=\"/home/pervinco/Datasets/PASCAL_VOC/VOCDevkit\", image_sets=['train', 'val'], years=['2007', '2012'], use_diff=False)\n",
    "print(len(train_dataset))\n",
    "\n",
    "data = train_dataset[0]\n",
    "image, boxes, classes = data[0], data[1], data[2]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 416, 416])\n",
      "torch.Size([4, 2, 4])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "def detection_collate(batch):\n",
    "    bsize = len(batch)\n",
    "    im_data, boxes, gt_classes, num_obj = zip(*batch)\n",
    "    max_num_obj = max([x.item() for x in num_obj]) ##  배치 내에서 가장 많은 객체를 가진 이미지를 기준으로 다른 이미지들의 객체 수를 맞춘다.\n",
    "    \n",
    "    ## 모든 이미지의 바운딩 박스와 클래스 레이블을 저장하기 위한 것으로, 초기에는 0으로 채워진다.\n",
    "    padded_boxes = torch.zeros((bsize, max_num_obj, 4))\n",
    "    padded_classes = torch.zeros((bsize, max_num_obj,))\n",
    "\n",
    "    for i in range(bsize):\n",
    "        padded_boxes[i, :num_obj[i], :] = boxes[i]\n",
    "        padded_classes[i, :num_obj[i]] = gt_classes[i]\n",
    "\n",
    "    return torch.stack(im_data, 0), padded_boxes, padded_classes, torch.stack(num_obj, 0)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, collate_fn=detection_collate, drop_last=True)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    images, boxes, classes, num_obj = data\n",
    "    print(images.shape)\n",
    "    print(boxes.shape)\n",
    "    print(classes.shape)\n",
    "    print(num_obj.shape)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
